[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Methods",
    "section": "",
    "text": "Preface\nThis is a book based on the classnotes of MATH 4833/5833",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html",
    "href": "Chapter5_SupportVectorMachine.html",
    "title": "1  Support Vector Machines",
    "section": "",
    "text": "1.1 Linear Support Vector Machines for Classification\nConsider a binary classification problem, where the data \\(D=\\{(\\boldsymbol{x}_1,y_1), (\\boldsymbol{x}_2,y_2), \\dots, (\\boldsymbol{x}_N,y_N)\\}\\), \\(y_i\\in \\{-1, 1\\}\\), are linearly separable. Our goal is to find a linear line (or a plane in 3D and a hyperplane in higher dimensions) that separates the data points. There can be infinitely many such choices, as shown in the figure below. In this example, the blue circles and red squares representing two different classes, with two features \\(\\boldsymbol{x}_1\\) and \\(\\boldsymbol{x}_2\\). Three lines, \\(l_1\\), \\(l_2\\) and \\(l_3\\), are plotted to show possible ways of separating the data linearly.\nWe need some criterion to decide the “best” separating line (or plane in higher dimensions), called the decision boundary. Intuitively, we want one that lies far from both datasets, so it has the best tolerance for perturbations/noises in the training data. Such a linear decision boundary can be described as \\[\\begin{equation*}\ns(\\boldsymbol{x}) = \\boldsymbol{w}^T\\boldsymbol{x} + b = 0\n\\end{equation*}\\] which is exactly the same form as those used for linear regression and logistic regression. Note that we separated the intercept \\(b\\) from the other coefficients \\(\\boldsymbol{w}\\) for an easier discussion of SVM. Assume a feature vector \\(\\boldsymbol{x} = (x_1, x_2, \\dots, x_d)^T\\) is \\(d\\)-dimensional and the training dataset comprises \\(N\\) instances \\(\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_N\\).\nSuppose the two classes are linearly separable, i.e., they can be clearly separated by the linear decision boundary. The class of points with label \\(+1\\) satisfies \\(s(\\boldsymbol{x})&gt;0\\) and the other class satisfies \\(s(\\boldsymbol{x})&lt;0\\). Predictions are made for new instances in the same fashion. Let \\(\\hat{y}(\\boldsymbol{x})\\) represent the predicted class for a new instance with features \\(\\boldsymbol{x}\\). We have\n\\[\\begin{equation}\n\\hat{y}(\\boldsymbol{x})=\\left\\{\\begin{array}{l l}\n-1 & \\text { if } s(\\boldsymbol{x})=\\boldsymbol{w}^{T} \\boldsymbol{x}+b&lt;0 \\\\\n1 & \\text { if } s(\\boldsymbol{x})=\\boldsymbol{w}^{T} \\boldsymbol{x}+b &gt; 0\n\\end{array}\\right.\n\\end{equation}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html#margin",
    "href": "Chapter5_SupportVectorMachine.html#margin",
    "title": "1  Support Vector Machines",
    "section": "1.2 Margin",
    "text": "1.2 Margin\nAs we saw in the figure above, there can be infinitely many ways to define a linear decision boundary. A natural way to select the “optimal” boundary is to find the boundary so that it is as far as possible from both classes of data points. Such a strategy can be described with the help of the concept of margin, which is defined as the minimum distance between the boundary and any data point. In the figure below, the margin \\(m\\), corresponding to the distance between boundary \\(l\\) and the data instance closest to \\(l\\) (the red square in the upper right corner), was plotted. An “optimal” boundary can then be defined as the one that has the maximum margin and such a classifier is called a maximum margin classifier. For example, \\(l^*\\) represents the maximum margin classifier, and \\(m^*\\) represents the maximum margin. Notice that data instances of the two classes that are closest to the boundary \\(l^*\\) are equidistant from it.\n\nWe now mathematically describe the maximum margin. It can be shown that the distance between an arbitrary point \\(\\boldsymbol{x}\\) in the feature space and the decision boundary \\(s(\\boldsymbol{x})=\\boldsymbol{w}^T\\boldsymbol{x}+b=0\\) is \\[\n\\frac{|s(\\boldsymbol{x})|}{||\\boldsymbol{w}||_2}\n\\] (see Excercise 5-1). Noting that \\(|s(\\boldsymbol{x})| = y(\\boldsymbol{x})s(\\boldsymbol{x})\\), the distance of \\(\\boldsymbol{x}_i\\) to the decision boundary can be rewritten as: \\[\\begin{equation}\n\\frac{y_is(\\boldsymbol{x}_i)}{||\\boldsymbol{w}||_2} = \\frac{y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b)}{||\\boldsymbol{w}||_2}\n\\end{equation}\\] Margin, defined as the smallest distance between the decision boundary and any data point \\(\\boldsymbol{x}_i\\), can be mathematically written as \\[\n\\text{margin} = \\min_{1\\le i\\le N}\\frac{y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b)}{||\\boldsymbol{w}||_2}=\\frac{1}{||\\boldsymbol{w}||_2}\\min_{1\\le i\\le N}y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b).\n\\] Therefore, the maximum margin is \\[\n\\max_{\\boldsymbol{w}, b}\\text{margin} = \\max_{\\boldsymbol{w}, b}\\left\\{\\frac{1}{||\\boldsymbol{w}||_2}\\min_{1\\le i\\le N}y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\right\\}\n\\] and our maximum margin classifier is found by computing \\[\n\\text{arg max}_{\\boldsymbol{w}, b}\\left\\{\\frac{1}{||\\boldsymbol{w}||_2}\\min_{1\\le i\\le N}y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\right\\}\n\\] An important fact related to the distance of \\(\\boldsymbol{x}_i\\) to the decision boundary is that it is invariant to the linear transformation (rescaling) \\(\\boldsymbol{w}\\rightarrow \\alpha\\boldsymbol{w}\\), and \\(b\\rightarrow \\alpha b\\) (but it does change the value of \\(y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b)\\); see Excercise 5-2). As a result, we can choose the \\(\\alpha\\) such that \\[\\begin{equation}\ny_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b) = 1\n\\end{equation}\\] for the points that are closest to the decision boundary (these points are also called support vectors, and hence the name support vector machines), as shown in the figure below:\n\nThen the maximum margin problem can be written as \\[\n\\text{arg max}_{\\boldsymbol{w}, b}\\frac{1}{||\\boldsymbol{w}||_2} = \\text{arg min}_{\\boldsymbol{w}, b}||\\boldsymbol{w}||_2^2 = \\text{arg min}_{\\boldsymbol{w}, b}\\frac{1}{2}||\\boldsymbol{w}||_2^2\n\\] where the last step is for the sake of simpler computation only. Now, the maximum margin criterion is to solve the following optimization problem: \\[\n\\begin{aligned}\n&\\text{arg min}_{\\boldsymbol{w}, b}\\frac{1}{2}||\\boldsymbol{w}||_2^2 \\\\\n&\\text{subject to } y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b) \\ge 1,\\quad 1\\le i\\le N\n\\end{aligned}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html#dual-problem",
    "href": "Chapter5_SupportVectorMachine.html#dual-problem",
    "title": "1  Support Vector Machines",
    "section": "1.3 Dual Problem",
    "text": "1.3 Dual Problem\nThe problem above is a quadratic programming problem, which (called the primal problem) can be converted to the dual problem by using the method of Lagrange multipliers. Using Lagrange multipliers \\(\\alpha_i\\ge 0\\) for each of the restrains, the Lagrangian function can be written as \\[\nL(\\boldsymbol{w}, b, \\boldsymbol{\\alpha}) = \\frac{1}{2}||\\boldsymbol{w}||_2^2 + \\sum_{i=1}^N \\alpha_i(1- y_i (\\boldsymbol{w}^T\\boldsymbol{x}_i+b))\n\\] where \\(\\boldsymbol{\\alpha} = (\\alpha_1,\\dots,\\alpha_N)\\). Taking derivative with respect to \\(\\boldsymbol{w}\\) and \\(b\\), and set them to \\(0\\). After substitution and organization (see Excercise 5-3), we have the dual problem: \\[\n\\begin{aligned}\n&\\text{arg}\\max_{\\boldsymbol{\\alpha}}L(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_i y_j \\boldsymbol{x}_{i}^T\\boldsymbol{x}_{j} \\\\\n&\\text{subject to } \\sum_{i=1}^N \\alpha_iy_i = 0,  \\quad \\alpha_i \\ge 0, \\,\\,\\, 1\\le i\\le N\n\\end{aligned}\n\\] After the dual problem is solved (numerically), for example, by the Sequential Minimal Optimization (SMO) algorithm (see Platt 1998), predictions for new data instances can be made by evaluating \\[\\begin{equation*}\ns(\\boldsymbol{x}) = \\boldsymbol{w}^T\\boldsymbol{x} + b = \\sum_{i=1}^N\\alpha_iy_i\\boldsymbol{x}_i^T\\boldsymbol{x} + b\n\\end{equation*}\\] We have not talked about how to compute \\(b\\) yet. Noting that for any support vector \\(\\boldsymbol{x}_s\\), the following equation is satisfied: \\[\\begin{equation*}\ny_is(\\boldsymbol{x}_s) = y_i\\left(\\sum_{i=1}^N\\alpha_iy_i\\boldsymbol{x}_i^T\\boldsymbol{x}_s + b\\right) = 1\n\\end{equation*}\\] and \\(b\\) can be obtained from any of these equations. A more numerically robust way to calculate \\(b\\) is to take the average of the \\(b\\)’s solved for from all these equations (see Excercise 5-4): \\[\\begin{equation*}\nb = \\frac{1}{N_{\\mathcal{S}}}\\sum_{i\\in \\mathcal{S}}\\left(y_i-\\sum_{j=1}^N\\alpha_jy_j\\boldsymbol{x}_j^T\\boldsymbol{x}_i\\right)\n\\end{equation*}\\] where \\(\\mathcal{S}\\) denotes the set of indices of the support vectors, and \\(N_{\\mathcal{S}}\\) is the total number of support vectors.\nExcercise 5-1\nShow that the distance between an arbitrary point \\(\\boldsymbol{x}\\) in the feature space and the decision boundary \\(s(\\boldsymbol{x})=\\boldsymbol{w}^T\\boldsymbol{x}+b=0\\) is \\[\n\\frac{|s(\\boldsymbol{x})|}{||\\boldsymbol{w}||_2}\n\\]\nExcercise 5-2\nShow that the distance of \\(\\boldsymbol{x}\\) to the decision boundary is invariant to the rescaling \\(\\boldsymbol{w}\\rightarrow \\alpha\\boldsymbol{w}\\), and \\(b\\rightarrow \\alpha b\\).\nExcercise 5-3\nDerive the dual problem from the primal problem for the maximum margin problem.\nExcercise 5-4\nShow the value of \\(b\\) can be computed by\n\\[\\begin{equation*}\n    b = \\frac{1}{N_{\\mathcal{S}}}\\sum_{i\\in \\mathcal{S}}\\left(y_i-\\sum_{j=1}^N\\alpha_jy_j\\boldsymbol{x}_j^T\\boldsymbol{x}_i\\right)\n\\end{equation*}\\]",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html#kernel-functions",
    "href": "Chapter5_SupportVectorMachine.html#kernel-functions",
    "title": "1  Support Vector Machines",
    "section": "1.4 Kernel Functions",
    "text": "1.4 Kernel Functions\nA binary classification problem may not be linearly separable. In such cases, there may exist maps such that the original feature space is projected to a higher dimension and the data points become linearly separable in that higher-dimensional space. Let such a map be \\(\\phi(\\boldsymbol{x})\\). All of the above formulas (\\(s(\\boldsymbol{x})\\), primal problem, and dual problem) need to be changed so that \\(\\boldsymbol{x}\\), \\(\\boldsymbol{x}_i\\), and \\(\\boldsymbol{x}_j\\) will be replaced by \\(\\phi(\\boldsymbol{x})\\), \\(\\phi(\\boldsymbol{x}_i)\\), and \\(\\phi(\\boldsymbol{x}_j)\\), respectively. The problem is computing inner products such as \\(\\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)\\) can be very expensive, especially considering that the dimension of the new feature space can be high, and even infinite. To avoid the direct calculation of the inner product, a trick is to imagine a function \\(k(\\boldsymbol{x}, \\boldsymbol{y})\\) exists and \\[\n\\begin{aligned}\nk(\\boldsymbol{x}_i, \\boldsymbol{x}_j) = \\phi(\\boldsymbol{x}_i)^T\\phi(\\boldsymbol{x}_j)\n\\end{aligned}\n\\] That is, there is a function that calculates the inner product of mapped features \\(\\phi(\\boldsymbol{x})\\) in the higher-dimensional space by evaluating a function defined in the original (low-dimensional) feature space. Such functions \\(k(\\cdot, \\cdot)\\) are called kernel functions. With the kernel function, the dual problem can be rewritten as \\[\n\\begin{aligned}\n&\\text{arg}\\max_{\\boldsymbol{\\alpha}}L(\\boldsymbol{\\alpha}) = \\sum_{i=1}^{N} \\alpha_{i}-\\frac{1}{2} \\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{i} \\alpha_{j} y_i y_j k(\\boldsymbol{x}_{i},\\boldsymbol{x}_{j}) \\\\\n&\\text{subject to } \\sum_{i=1}^N \\alpha_iy_i = 0,  \\quad \\alpha_i \\ge 0, \\,\\,\\, 1\\le i\\le N\n\\end{aligned}\n\\] and the linear function \\(s(\\boldsymbol{x})\\) becomes \\[\\begin{equation*}\ns(\\boldsymbol{x}) = \\sum_{i=1}^N\\alpha_iy_ik(\\boldsymbol{x}_i,\\boldsymbol{x}) + b\n\\end{equation*}\\] Since, we are not sure what \\(\\phi(\\cdot)\\) looks like, and thus finding the kernel function corresponding to \\(\\phi(\\cdot)\\) is not possible. As a result, we often turn to some commonly used kernel functions as shown below, and in fact, each of them implicitly defines a map \\(\\phi(\\cdot)\\). \\[\n\\begin{aligned}\n\\text{ Linear: } k(\\boldsymbol{x}, \\boldsymbol{y}) &= \\boldsymbol{x}^T\\boldsymbol{y} \\\\\n\\text { Polynomial: } k(\\boldsymbol{x}, \\boldsymbol{y}) &= \\left(\\gamma \\boldsymbol{x}^{T} \\boldsymbol{y}+r\\right)^{d} \\\\\n\\text { Gaussian RBF: } k(\\boldsymbol{x}, \\boldsymbol{y}) &= \\exp \\left(-\\gamma\\|\\boldsymbol{x}-\\boldsymbol{y}\\|_2^{2}\\right) \\\\\n\\text { Sigmoid: } k(\\boldsymbol{x}, \\boldsymbol{y}) &= \\tanh \\left(\\gamma \\boldsymbol{x}^{T} \\boldsymbol{y}+r\\right) \\\\\n\\text{ Laplacian: } k(\\boldsymbol{x}, \\boldsymbol{y}) &= \\exp \\left(-\\gamma\\|\\boldsymbol{x}-\\boldsymbol{y}\\|_1\\right)\n\\end{aligned}\n\\] These standard kernel functions can be combined to form new kernel functions. For instance, a linear combination of kernel functions is still a kernel function.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html#soft-margin",
    "href": "Chapter5_SupportVectorMachine.html#soft-margin",
    "title": "1  Support Vector Machines",
    "section": "1.5 Soft Margin",
    "text": "1.5 Soft Margin\nEven if the data points are linearly separable (in the original feature space or with a kernel function), it may still not be a good idea to use such a decision boundary, because of overfitting and poor generalization. To alleviate this problem, we may consider a more flexible model that allows data points to appear on the wrong side of the decision boundary (\\(y_is(\\boldsymbol{x}_i)&lt; 0\\)), while keeping the margin as large as possible. As the figure shown below, there are two instances of the blue circle class (target \\(+1\\)) on the wrong side, and one instance of the red square class (target \\(-1\\)). All of them are colored slightly differently in the figure. Data points can also be inside the margin boundary, although it is on the right side of the decision boundary (there is one such point in the red square class in the figure). Such a method is called soft margin classification.\n\nA possible cost function to achieve a balance of maximum margin and limiting cases of being on the wrong side and inside the margin boundary (characterized by \\(y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)&lt;1\\)) is:\n\\[\\begin{equation*}\n\\text{arg min}_{\\boldsymbol{w}, b}\\frac{1}{2}||\\boldsymbol{w}||_2^2 + C\\sum_{i=1}^N(1-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b))^+\n\\end{equation*}\\]\nwhere \\(C&gt;0\\) is a constant that controls the penalty to be applied for the instances that result in violations, and \\((\\cdot)^+\\) is the Heaviside step function with \\((0)^+=0\\). However, it is nonconvex and not continuous, leading to difficulty in solving the optimization problem. Other functions can be used to replace the Heaviside step function. One example is the hinge loss function:\n\\[\\begin{equation*}\nL_{\\text{hinge}}(x) = \\max(0, 1-x)\n\\end{equation*}\\]\n\n# Plot of a hinge function\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nx = np.linspace(-2, 4, 100)\ny = np.zeros(100)\nfor i in range(100):\n    y[i] = max(0, 1-x[i])\nplt.plot(x, y, 'b-')\nplt.xlabel('x')\nplt.title('Hinge Loss');\n\n\n\n\n\n\n\n\nWith the hinge loss, the soft margin objective function can be written as: \\[\n\\begin{aligned}\n&\\text{arg min}_{\\boldsymbol{w}, b}\\frac{1}{2}||\\boldsymbol{w}||_2^2 + C\\sum_{i=1}^N L_{\\text{hinge}}(y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b)) \\\\\n&= \\text{arg min}_{\\boldsymbol{w}, b}\\frac{1}{2}||\\boldsymbol{w}||_2^2 + C\\sum_{i=1}^N\\max(0, 1-y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b))\n\\end{aligned}\n\\]\nThe hinge loss function penalizes more those points farther away from the boundary. Introduce slack variables \\(\\xi_i\\ge 0\\), \\(1\\le i\\le N\\), and \\(\\xi_i = L_{\\text{hinge}}(y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b))=|y_i-s(\\boldsymbol{x}_i)|\\). The objective function can be rewritten as \\[\n\\begin{aligned}\n&\\text{arg min}_{\\boldsymbol{w}, b,\\varepsilon_i}\\frac{1}{2}||\\boldsymbol{w}||_2^2 + C\\sum_{i=1}^N\\varepsilon_i \\\\\n&\\text{subject to } y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b) \\ge 1-\\varepsilon_i,\\quad \\varepsilon_i\\ge 0,\\, i=1,\\dots, N\n\\end{aligned}\n\\] This is still a quadratic problem and similarly a dual problem can be obtained using Lagrange multiplier.\nExcercise 5-5\nShow the constraints \\(y_i(\\boldsymbol{w}^T\\boldsymbol{x}_i+b) \\ge 1-\\varepsilon_i\\), \\(i=1,\\dots, N\\) need to hold for the soft margin classification problem.\nExample 5-1\nBuild a binary SVM classifier for the Iris Dataset to classify virginica and non-virginica. Consider two cases: 1) using sepal length and petal width as features; 2) using petal length and petal width as features.\n\nfrom sklearn import datasets\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\n\n# Load the data\niris = datasets.load_iris()\n# Show what the iris dataset look like\nprint(iris)\n# Use sepal length and petal width as features\nX = iris[\"data\"][:, [0, 3]]  # Sepal length and petal width\n# 1 if Iris-Virginica, else 0\ny = (iris[\"target\"] == 2).astype('int')\n# Build linear svm classifier\n# Scaling is important for SVM.\n# Build a Pipeline that first scales data and then trains the model\n# For hard margin, set C=0.0\nsvm_clf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"linear_svc\", LinearSVC(C=1.0, loss=\"hinge\", dual=\"auto\", random_state=36)),\n    ])\n# Train the model\nsvm_clf.fit(X, y);\n\n{'data': array([[5.1, 3.5, 1.4, 0.2],\n       [4.9, 3. , 1.4, 0.2],\n       [4.7, 3.2, 1.3, 0.2],\n       [4.6, 3.1, 1.5, 0.2],\n       [5. , 3.6, 1.4, 0.2],\n       [5.4, 3.9, 1.7, 0.4],\n       [4.6, 3.4, 1.4, 0.3],\n       [5. , 3.4, 1.5, 0.2],\n       [4.4, 2.9, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.1],\n       [5.4, 3.7, 1.5, 0.2],\n       [4.8, 3.4, 1.6, 0.2],\n       [4.8, 3. , 1.4, 0.1],\n       [4.3, 3. , 1.1, 0.1],\n       [5.8, 4. , 1.2, 0.2],\n       [5.7, 4.4, 1.5, 0.4],\n       [5.4, 3.9, 1.3, 0.4],\n       [5.1, 3.5, 1.4, 0.3],\n       [5.7, 3.8, 1.7, 0.3],\n       [5.1, 3.8, 1.5, 0.3],\n       [5.4, 3.4, 1.7, 0.2],\n       [5.1, 3.7, 1.5, 0.4],\n       [4.6, 3.6, 1. , 0.2],\n       [5.1, 3.3, 1.7, 0.5],\n       [4.8, 3.4, 1.9, 0.2],\n       [5. , 3. , 1.6, 0.2],\n       [5. , 3.4, 1.6, 0.4],\n       [5.2, 3.5, 1.5, 0.2],\n       [5.2, 3.4, 1.4, 0.2],\n       [4.7, 3.2, 1.6, 0.2],\n       [4.8, 3.1, 1.6, 0.2],\n       [5.4, 3.4, 1.5, 0.4],\n       [5.2, 4.1, 1.5, 0.1],\n       [5.5, 4.2, 1.4, 0.2],\n       [4.9, 3.1, 1.5, 0.2],\n       [5. , 3.2, 1.2, 0.2],\n       [5.5, 3.5, 1.3, 0.2],\n       [4.9, 3.6, 1.4, 0.1],\n       [4.4, 3. , 1.3, 0.2],\n       [5.1, 3.4, 1.5, 0.2],\n       [5. , 3.5, 1.3, 0.3],\n       [4.5, 2.3, 1.3, 0.3],\n       [4.4, 3.2, 1.3, 0.2],\n       [5. , 3.5, 1.6, 0.6],\n       [5.1, 3.8, 1.9, 0.4],\n       [4.8, 3. , 1.4, 0.3],\n       [5.1, 3.8, 1.6, 0.2],\n       [4.6, 3.2, 1.4, 0.2],\n       [5.3, 3.7, 1.5, 0.2],\n       [5. , 3.3, 1.4, 0.2],\n       [7. , 3.2, 4.7, 1.4],\n       [6.4, 3.2, 4.5, 1.5],\n       [6.9, 3.1, 4.9, 1.5],\n       [5.5, 2.3, 4. , 1.3],\n       [6.5, 2.8, 4.6, 1.5],\n       [5.7, 2.8, 4.5, 1.3],\n       [6.3, 3.3, 4.7, 1.6],\n       [4.9, 2.4, 3.3, 1. ],\n       [6.6, 2.9, 4.6, 1.3],\n       [5.2, 2.7, 3.9, 1.4],\n       [5. , 2. , 3.5, 1. ],\n       [5.9, 3. , 4.2, 1.5],\n       [6. , 2.2, 4. , 1. ],\n       [6.1, 2.9, 4.7, 1.4],\n       [5.6, 2.9, 3.6, 1.3],\n       [6.7, 3.1, 4.4, 1.4],\n       [5.6, 3. , 4.5, 1.5],\n       [5.8, 2.7, 4.1, 1. ],\n       [6.2, 2.2, 4.5, 1.5],\n       [5.6, 2.5, 3.9, 1.1],\n       [5.9, 3.2, 4.8, 1.8],\n       [6.1, 2.8, 4. , 1.3],\n       [6.3, 2.5, 4.9, 1.5],\n       [6.1, 2.8, 4.7, 1.2],\n       [6.4, 2.9, 4.3, 1.3],\n       [6.6, 3. , 4.4, 1.4],\n       [6.8, 2.8, 4.8, 1.4],\n       [6.7, 3. , 5. , 1.7],\n       [6. , 2.9, 4.5, 1.5],\n       [5.7, 2.6, 3.5, 1. ],\n       [5.5, 2.4, 3.8, 1.1],\n       [5.5, 2.4, 3.7, 1. ],\n       [5.8, 2.7, 3.9, 1.2],\n       [6. , 2.7, 5.1, 1.6],\n       [5.4, 3. , 4.5, 1.5],\n       [6. , 3.4, 4.5, 1.6],\n       [6.7, 3.1, 4.7, 1.5],\n       [6.3, 2.3, 4.4, 1.3],\n       [5.6, 3. , 4.1, 1.3],\n       [5.5, 2.5, 4. , 1.3],\n       [5.5, 2.6, 4.4, 1.2],\n       [6.1, 3. , 4.6, 1.4],\n       [5.8, 2.6, 4. , 1.2],\n       [5. , 2.3, 3.3, 1. ],\n       [5.6, 2.7, 4.2, 1.3],\n       [5.7, 3. , 4.2, 1.2],\n       [5.7, 2.9, 4.2, 1.3],\n       [6.2, 2.9, 4.3, 1.3],\n       [5.1, 2.5, 3. , 1.1],\n       [5.7, 2.8, 4.1, 1.3],\n       [6.3, 3.3, 6. , 2.5],\n       [5.8, 2.7, 5.1, 1.9],\n       [7.1, 3. , 5.9, 2.1],\n       [6.3, 2.9, 5.6, 1.8],\n       [6.5, 3. , 5.8, 2.2],\n       [7.6, 3. , 6.6, 2.1],\n       [4.9, 2.5, 4.5, 1.7],\n       [7.3, 2.9, 6.3, 1.8],\n       [6.7, 2.5, 5.8, 1.8],\n       [7.2, 3.6, 6.1, 2.5],\n       [6.5, 3.2, 5.1, 2. ],\n       [6.4, 2.7, 5.3, 1.9],\n       [6.8, 3. , 5.5, 2.1],\n       [5.7, 2.5, 5. , 2. ],\n       [5.8, 2.8, 5.1, 2.4],\n       [6.4, 3.2, 5.3, 2.3],\n       [6.5, 3. , 5.5, 1.8],\n       [7.7, 3.8, 6.7, 2.2],\n       [7.7, 2.6, 6.9, 2.3],\n       [6. , 2.2, 5. , 1.5],\n       [6.9, 3.2, 5.7, 2.3],\n       [5.6, 2.8, 4.9, 2. ],\n       [7.7, 2.8, 6.7, 2. ],\n       [6.3, 2.7, 4.9, 1.8],\n       [6.7, 3.3, 5.7, 2.1],\n       [7.2, 3.2, 6. , 1.8],\n       [6.2, 2.8, 4.8, 1.8],\n       [6.1, 3. , 4.9, 1.8],\n       [6.4, 2.8, 5.6, 2.1],\n       [7.2, 3. , 5.8, 1.6],\n       [7.4, 2.8, 6.1, 1.9],\n       [7.9, 3.8, 6.4, 2. ],\n       [6.4, 2.8, 5.6, 2.2],\n       [6.3, 2.8, 5.1, 1.5],\n       [6.1, 2.6, 5.6, 1.4],\n       [7.7, 3. , 6.1, 2.3],\n       [6.3, 3.4, 5.6, 2.4],\n       [6.4, 3.1, 5.5, 1.8],\n       [6. , 3. , 4.8, 1.8],\n       [6.9, 3.1, 5.4, 2.1],\n       [6.7, 3.1, 5.6, 2.4],\n       [6.9, 3.1, 5.1, 2.3],\n       [5.8, 2.7, 5.1, 1.9],\n       [6.8, 3.2, 5.9, 2.3],\n       [6.7, 3.3, 5.7, 2.5],\n       [6.7, 3. , 5.2, 2.3],\n       [6.3, 2.5, 5. , 1.9],\n       [6.5, 3. , 5.2, 2. ],\n       [6.2, 3.4, 5.4, 2.3],\n       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'frame': None, 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='&lt;U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 150 (50 in each of three classes)\\n:Number of Attributes: 4 numeric, predictive attributes and the class\\n:Attribute Information:\\n    - sepal length in cm\\n    - sepal width in cm\\n    - petal length in cm\\n    - petal width in cm\\n    - class:\\n            - Iris-Setosa\\n            - Iris-Versicolour\\n            - Iris-Virginica\\n\\n:Summary Statistics:\\n\\n============== ==== ==== ======= ===== ====================\\n                Min  Max   Mean    SD   Class Correlation\\n============== ==== ==== ======= ===== ====================\\nsepal length:   4.3  7.9   5.84   0.83    0.7826\\nsepal width:    2.0  4.4   3.05   0.43   -0.4194\\npetal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\npetal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n============== ==== ==== ======= ===== ====================\\n\\n:Missing Attribute Values: None\\n:Class Distribution: 33.3% for each of 3 classes.\\n:Creator: R.A. Fisher\\n:Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n:Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|\\n', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': 'iris.csv', 'data_module': 'sklearn.datasets.data'}\n\n\n\n# Plot the decision boundary\n\nX_scaled = svm_clf[\"scaler\"].transform(X)\nx_db = [X_scaled[:,0].min(), X_scaled[:,0].max()]\nbeta0 = svm_clf[\"linear_svc\"].intercept_[0]\nbeta1 = svm_clf[\"linear_svc\"].coef_[0][0]\nbeta2 = svm_clf[\"linear_svc\"].coef_[0][1]\ny_db = -(beta0 + np.dot(beta1, x_db)) / beta2\nplt.scatter(X_scaled[iris[\"target\"] == 2, 0], X_scaled[iris[\"target\"] == 2, 1],\n            marker='^', c='g', s=24, label='Irs-Virginica')\nplt.scatter(X_scaled[iris[\"target\"] != 2, 0], X_scaled[iris[\"target\"] != 2, 1],\n            marker='s', c='b', s=24, label='Not Iris-Virginica')\nplt.plot(x_db, y_db, label='Decision Boundary', c='r')\nplt.legend(fontsize=16, loc='lower right')\nplt.xlabel('Sepal length', fontsize=16)\nplt.ylabel('Petal width', fontsize=16);\n\n\n\n\n\n\n\n\n\n# Use petal length and width as features\nX = iris[\"data\"][:, 2:]  # petal length and width\n# Train the model\nsvm_clf.fit(X, y)\n\n# Plot the decision boundary\n\nX_scaled = svm_clf[\"scaler\"].transform(X)\nx_db = [X_scaled[:,0].min(), X_scaled[:,0].max()]\nbeta0 = svm_clf[\"linear_svc\"].intercept_[0]\nbeta1 = svm_clf[\"linear_svc\"].coef_[0][0]\nbeta2 = svm_clf[\"linear_svc\"].coef_[0][1]\ny_db = -(beta0 + np.dot(beta1, x_db)) / beta2\nplt.scatter(X_scaled[iris[\"target\"] == 2, 0], X_scaled[iris[\"target\"] == 2, 1],\n            marker='^', c='g', s=24, label='Irs-Virginica')\nplt.scatter(X_scaled[iris[\"target\"] != 2, 0], X_scaled[iris[\"target\"] != 2, 1],\n            marker='s', c='b', s=24, label='Not Iris-Virginica')\nplt.plot(x_db, y_db, label='Decision Boundary', c='r')\nplt.legend(fontsize=16, loc='lower right')\nplt.xlabel('Petal length', fontsize=16)\nplt.ylabel('Petal width', fontsize=16);\n\n\n\n\n\n\n\n\nExample 5-2\nConsider isotropic Gaussian blobs for binary classification (use sklearn.datasets.make_blobs). Use a sigmoid kernel with \\(\\gamma\\) set to “scale” to train a SVM.\n\nfrom sklearn.svm import SVC\n\n# Generate two isotropic Gaussian blobs with two features, each blob has 200 data points\nX, y = datasets.make_blobs(n_samples=200, n_features=2, centers=2, random_state=3)\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=16)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\nplt.title('Two isotropic Gaussian blobs');\n\n\n\n\n\n\n\n\n\n# Train SVM with a sigmoid kernel\nsvm_classifier = SVC(kernel='sigmoid', gamma='scale')\nsvm_classifier.fit(X, y)\n\n# Plot the decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min(), X[:,0].max(), 400)\nx2 = np.linspace(X[:,1].min(), X[:,1].max(), 400)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = svm_classifier.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', c='b', s=24, label='Label 0')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', c='g', s=24, label='Label 1')\nplt.legend(fontsize=16)\nplt.xlabel('$X_1$', fontsize=16)\nplt.ylabel('$X_2$', fontsize=16);\n\n\n\n\n\n\n\n\nExample 5-3\nTrain an SVM for two circles (sklearn.datasets.make_circles) using a Gaussin rbf kernel with \\(\\gamma=0.7\\).\n\n# Create two circles with a total number of 200 points, \n# a scale factor of 0.3 between inner and outer circle in the range [0, 1)\n# and a standard deviation of 0.1 of Gaussian noise added to the data.\nX, y = datasets.make_circles(n_samples=100, factor=0.3, noise=0.1, random_state=10)\n\nplt.scatter(X[:, 0], X[:, 1], marker='o', c=y, s=16)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\nplt.title('Two circles');\n\n# Train SVM with a sigmoid kernel\nsvm_classifier = SVC(kernel='rbf', gamma=0.7)\nsvm_classifier.fit(X, y);\n\n\n\n\n\n\n\n\n\n# Plot the decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min(), X[:,0].max(), 400)\nx2 = np.linspace(X[:,1].min(), X[:,1].max(), 400)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = svm_classifier.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', c='b', s=24, label='Label 0')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', c='g', s=24, label='Label 1')\nplt.legend(fontsize=16)\nplt.xlabel('$X_1$', fontsize=16)\nplt.ylabel('$X_2$', fontsize=16);\n\n\n\n\n\n\n\n\n\n1.5.1 SVM Regression\nThe SVM algorithm can handle both linear and nonlinear classification, as well as linear and nonlinear regression. In SVM for regression, the aim is to fit as many data points as possible within a margin, while minimizing the number of data points that fall outside the margin (see figure below; the points located outside of the margin are colored differently). The width of this margin boundary is controlled by a hyperparameter \\(\\epsilon\\). The following figure illustrates a linear SVM Regression model with a hyperparameter \\(\\epsilon\\). The idea is that we can tolerate a deviation that is less than \\(\\epsilon\\) for a prediction \\(f(\\boldsymbol{x})-y\\), where \\(f(\\boldsymbol{x})\\) is the SVM regression prediction, and \\(y\\) is the target corresponding to feature \\(\\boldsymbol{x}\\). There will be a penalty when the deviation is greater than \\(\\epsilon\\). The SVR regression problem can thus be described mathematically as:\n\\[\\begin{equation*}\n\\min_{\\boldsymbol{w},b}\\frac{1}{2}\\|\\boldsymbol{w}\\|^2 + C\\sum_{i=1}^NL_{\\epsilon}(f(\\boldsymbol{x}_i)-y_i)\n\\end{equation*}\\]\nwhere \\(L_{\\epsilon}\\) is the \\(\\epsilon\\)-insensitive loss function defined as:\n\\[\\begin{equation*}\nL_{\\epsilon}(x) =\n\\begin{cases}\n0, & \\text{if } |x|\\le \\epsilon \\\\\n|x|-\\epsilon, & \\text{otherwise}\n\\end{cases}\n\\end{equation*}\\]\n\nScikit-Learn’s LinearSVR class can perform linear SVM Regression, and SVR can perform kernelized SVM model for nonlinear regression tasks.\nExample 5-4\nConsider the function \\(y=f(x)=\\sin(x)\\). Create a dataset of \\(50\\) instances with \\(x\\in (0,5)\\) and Build SVM regression models based on the data using three kernels: linear, rbf and polynomial of degree 3.\n\nfrom sklearn.svm import SVR\n\nndata = 50\n# Create the training dataset\nnp.random.seed(20)\nX = np.sort(5 * np.random.rand(ndata))\ny = np.sin(X)\nX = X[:, np.newaxis]\n\n# Fit SVR models\nsvr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=0.1)\nsvr_lin = SVR(kernel='linear', C=100, epsilon=0.1)\nsvr_poly = SVR(kernel='poly', C=100, degree=3, epsilon=0.1, coef0=1)\n\n# Train the SVR models\ny_rbf = svr_rbf.fit(X, y).predict(X)\ny_lin = svr_lin.fit(X, y).predict(X)\ny_poly = svr_poly.fit(X, y).predict(X)\n\n# Plot the original data with the three models\nplt.figure(figsize=(10, 6))\nplt.scatter(X, y, color='darkorange', label='Data')\nplt.plot(X, y_rbf, color='navy', lw=2, label='RBF model')\nplt.plot(X, y_lin, color='c', lw=2, label='Linear model')\nplt.plot(X, y_poly, color='cornflowerblue', lw=2, label='Polynomial model')\nplt.xlabel('X')\nplt.ylabel('y')\nplt.title('Support Vector Regression for $y=\\sin(x)$')\nplt.legend();",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter5_SupportVectorMachine.html#references",
    "href": "Chapter5_SupportVectorMachine.html#references",
    "title": "1  Support Vector Machines",
    "section": "1.6 References",
    "text": "1.6 References\n\nCortes, C. and Vapnik, V.N., Support vector networks. Machine Learning, 20(3): 273-297.\nPlatt, J.C., Sequential Minimal Optimization: A fast algorithm for training support vector machines. Technical Report MSR-TR-98-14, Microsoft Research, 1998.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "Chapter6_DecisionTrees.html",
    "href": "Chapter6_DecisionTrees.html",
    "title": "2  Decision Trees",
    "section": "",
    "text": "2.1 Decision Trees for Classification\nTo train a decision tree, the main goal is to decide which feature to choose at a node and at what value of the feature the node is divided. To this end, we need to define a measure that can be used to compare different choices and values of features for divisions. A greedy optimization approach is typically used, beginning with a single root node that represents the entire training dataset. The tree is then grown incrementally by adding nodes one at a time. At each step, there are a set of candidate features and their values in the input space for the split, and once they are chosen, a pair of leaf nodes will be added to the current tree corresponding to the split. Let \\(\\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_N, y_N)\\}\\) be a training dataset, where \\(y_i\\in\\{1,2,\\dots, K\\}\\), \\(1\\le i\\le N\\). Assume at a certain step, there has been a collection of leaf nodes \\(L=\\{L_1, L_2,\\dots, L_M\\}\\), For each of the leaf nodes \\(L_i\\), and a candidate criterion for the split, \\(c\\), there will be two children for \\(L_i\\), denoted by \\(L_i^0(c)\\) and \\(L_i^1(c)\\). To quantify the performance of the split, there are two commonly used measures, namely, information gain and Gini index (also known as Gini impurity). To define information gain, we first define the information entropy (or just entropy) as\n\\[\\begin{equation*}\n\\Eta(L_i) = -\\sum_{k=1}^Kp_k\\log{p_k}\n\\end{equation*}\\] where \\(p_k, k=1,\\dots,K\\) is the proportion of data points in \\(L_i\\) that belong to class \\(k\\), and \\(\\log\\) is the natural logarithm (base 2 is also commonly used). For each of the two children of \\(L_i\\), we can compute \\(\\Eta(L_i^0(c))\\) and \\(\\Eta(L_i^1(c))\\) in the same way. The information gain can then be defined as the difference between the entropy of \\(L_i\\) and the weighted some of the entropies of its two children:\n\\[\\begin{equation*}\n\\text{IG}(L_i, c) = \\Eta(L_i) - \\sum_{k=0}^1 \\frac{|L_i^k(c)|}{|L_i|}\\Eta(L_i^k(c))\n\\end{equation*}\\]\nwhere \\(|\\cdot|\\) denotes the number of data points associated with a node. Hence the weights are simply the proportion of data points in the children to the total number of instances in the parent. The larger the information gain, the purer the two children resulting from a split. Notice that if a split results in two pure children, then both children nodes have an entropy of \\(0\\). If a node has the same proportion of each class (i.e., \\(p_k=\\frac{1}{K}\\), \\(1\\le k\\le K\\)), then its entropy achieves a maximum (see Exercise 6-1).\nAnother way to measure a potential split criterion \\(c\\) is to evaluate the Gini index, which is defined as\n\\[\\begin{equation*}\nG(L_i) = \\sum_{k=0}^Kp_k(1-p_k) = 1-\\sum_{k=0}^Kp_k^2\n\\end{equation*}\\]\n\\(G(L_i)\\) will be equal to \\(0\\) if \\(L_i\\) is pure, and it is largest when each class has the same proportion of the data instances (see Exercise 6-2). To evaluate the performance of the split criterion \\(c\\), we calculate the weighted Gini index for the two children nodes:\n\\[\\begin{equation*}\nG(L_i, c) = \\sum_{k=0}^1\\frac{|L_i^k(c)|}{|L_i|}G(L_i^k(c))\n\\end{equation*}\\]\nand we pick the best split that leads to the smallest \\(G(L_i, c)\\). The process continues until certain stopping criterion is satisfied, for example, all leaf nodes are pure, or the depth of the tree (number of nodes along the longest path from the root node down to the farthest leaf node) reaches a preset value.\nNotice that the algorithm works for both discrete and continuous features. For the latter, a discretization of the feature space needs to be performed before applying the algorithm.\nExercise 6-1\nExercise 6-2",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "Chapter6_DecisionTrees.html#decision-trees-for-classification",
    "href": "Chapter6_DecisionTrees.html#decision-trees-for-classification",
    "title": "2  Decision Trees",
    "section": "",
    "text": "Verify that a node’s information entropy achieves its maximum if each of the \\(K\\) classes has the same proportion of its data instances.\nVerify that a pure node’s information entropy is \\(0\\).\n\n\n\nVerify that a node’s Gini index achieves its maximum if each of the \\(K\\) classes has the same proportion of the data instances.\nVerify that a pure node’s Gini index is \\(0\\).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "Chapter6_DecisionTrees.html#decision-trees-for-regression",
    "href": "Chapter6_DecisionTrees.html#decision-trees-for-regression",
    "title": "2  Decision Trees",
    "section": "2.2 Decision Trees for Regression",
    "text": "2.2 Decision Trees for Regression\nThe algorithm for building a decision tree for regression is similar to that for classification. The only difference is that we need to define a new performance measure for a split. Given the training dataset \\(\\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_N, y_N)\\}\\), assume we have obtained a collection of leaf nodes \\(L=\\{L_1, L_2,\\dots, L_M\\}\\). For a node \\(L_i\\) and a candidate criterion for the split, \\(c\\), the resulting two children nodes are \\(L_i^0(c)\\) and \\(L_i^1(c)\\). Predictions of the target corresponding to a children node should be made by taking the average of all the \\(y_i\\)’s of those data points located in the node, which is equivalent to minimizing the sum-of-squares error in the node (see Excercise 6-3). Then, the optimal choice of \\(c\\) will be the one that leads to the smallest residual sum-of-squares error.\nTo stop the algorithm, we impose some stopping criterion, e.g., the reduction of residual sum-of-squares error falls below some threshold, or another one mentioned in the case of classification.\nThe following figure shows such an example, where \\(\\boldsymbol{x} = (x_1,x_2)\\), i.e., the feature space is two-dimensional. The first step splits the entire feature space into two regions depending on \\(x_1&lt;x_{11}\\) or not; the second steps splits the region \\(\\{(x,y)|x&lt;x_{11}\\}\\) into two subregions labeled 1 and 2, depending on \\(x_2&lt;x_{21}\\) or not; the third step divides the region \\(\\{(x,y)|x&gt;x_{11}\\}\\) into two subregions depending on \\(x_2&lt;x_{22}\\) or not, and the resulting two subregions are further divided into regions 3 and 4, and 5 and 6, respectively, depending on if \\(x_1&lt;x_{12}\\) and \\(x_1&lt;x_{13}\\).\n\nExercise 6-3\nShow that taking the average of all the \\(y_i\\)’s of the data points located in a node as the predicted target is equivalent to minimizing the sum-of-squares error in the node.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "Chapter6_DecisionTrees.html#tree-pruning",
    "href": "Chapter6_DecisionTrees.html#tree-pruning",
    "title": "2  Decision Trees",
    "section": "2.3 Tree Pruning",
    "text": "2.3 Tree Pruning\nTo avoid overfitting and increase generalization of decision trees, in the training process, it is a common practice to build a deep tree (e.g., to its maximum depth), and then trim the branches that contribute the least to the model’s predictive power. One commonly used method is the weakest link tree pruning method (also known as cost complexity pruning) Assume a large tree has been built. To apply weakest link tree pruning, we evaluate each subtree in the decision tree for its contribution to the tree’s performance, for example, using the misclassification rate for classification trees or residual sum-of-squares for regressin trees. Once we identify the “weakest link”, i.e., the subtree, if removed, that leads to the smallest increase in the error (misclassification rate or residual sum-of-squares), the weakest link is pruned (removed) from the tree. The process is repeated iteratively until a stopping criterion is satisfied, e.g. the smallest increase in error by removing a weakest link is bigger than a threshold.\nExample 6-1\nBuild a decision tree classifier for the Iris Dataset to classify virginica and non-virginica. Use petal length and petal width as features.\n\nimport numpy as np\nfrom sklearn import datasets\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport graphviz\nimport pydotplus\nfrom IPython.display import Image\n\n# Load the data\niris = datasets.load_iris()\n# Use sepal length and width as features\nX = iris[\"data\"][:, 2:] # sepal length and width\n# 0: 'setosa', 1: 'versicolor', 2: 'virginica'\ny = iris.target\n# Build a decision tree. By default, it uses Gini index\ntree_clf = DecisionTreeClassifier(max_depth=2, random_state=32)\ntree_clf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=2, random_state=32)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=2, random_state=32) \n\n\n\n# Visualize the trained decision tree\n\n# Export the tree to a dot file\ndot_data = export_graphviz(tree_clf, out_file=None,\n                           feature_names=iris.feature_names[2:],\n                           class_names=iris.target_names, rounded=True,\n                           filled=True)\n\n# Convert the .dot file to a graph\ngraph = pydotplus.graph_from_dot_data(dot_data)\n\n# Show the graph\nImage(graph.create_png())\n\n\n\n\n\n\n\n\n\n# plot decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100)\nx2 = np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = tree_clf.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[iris[\"target\"] == 0, 0], X[iris[\"target\"] == 0, 1],\nmarker='o', c='m', s=24, label='Iris-Setosa')\nplt.scatter(X[iris[\"target\"] == 1, 0], X[iris[\"target\"] == 1, 1],\nmarker='s', c='b', s=24, label='Iris-Versicolor')\nplt.scatter(X[iris[\"target\"] == 2, 0], X[iris[\"target\"] == 2, 1],\nmarker='^', c='g', s=24, label='Iris-Virginica')\nplt.legend(fontsize=16)\nplt.xlabel('Petal length', fontsize=16)\nplt.ylabel('Petal width', fontsize=16);\n\n\n\n\n\n\n\n\nIn addition, we can estimate the class probabilities for a prediction based on the proportion of data points belonging to the leaf node. For example,\n\ntree_clf.predict_proba([[4.5, 1.6]])\n\narray([[0.        , 0.90740741, 0.09259259]])\n\n\nshows that in the leaf node where the data instance \\(\\boldsymbol{x}=(4.5, 1.6)\\) fall, there are no Setosa cases, and there are 90.1% Versicolor cases, and 9.3% Virginica cases.\nExample 6-2\nConsider a binary classification problem where the data are generate with sklearn.datasets.make_classification.\n\nfrom sklearn.datasets import make_classification\n\n# Generate a synthetic dataset using make_classification with 2 features\nX, y = make_classification(n_samples=100, n_features=2, n_informative=2, n_redundant=0, \n                           n_clusters_per_class=1, random_state=21)\n\n\n# Visualize the data\n\nplt.scatter(X[:,0], X[:,1], c=y, s=16)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16)\nplt.title('Synthetic data for binary classification');\n\n\n\n\n\n\n\n\n\n# Train a Decision Tree Classifier without regularization\nclf = DecisionTreeClassifier(random_state=20)\nclf.fit(X, y)\n\nDecisionTreeClassifier(random_state=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(random_state=20) \n\n\n\n# plot decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100)\nx2 = np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = clf.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', c='m', s=24, label='Label 0')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', c='b', s=24, label='Label 1')\nplt.legend(fontsize=16)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16);\n\n\n\n\n\n\n\n\nClearly, the model is overfitting. Now we apply one of the regularization measures for decision trees.\n\n# Train a Decision Tree Classifier with regularization\nclf = DecisionTreeClassifier(random_state=20, max_depth=4)\nclf.fit(X, y)\n\nDecisionTreeClassifier(max_depth=4, random_state=20)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeClassifier?Documentation for DecisionTreeClassifieriFittedDecisionTreeClassifier(max_depth=4, random_state=20) \n\n\n\n# plot decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100)\nx2 = np.linspace(X[:,1].min()-0.1, X[:,1].max()+0.1, 100)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = clf.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[y == 0, 0], X[y == 0, 1], marker='o', c='m', s=24, label='Label 0')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], marker='s', c='b', s=24, label='Label 1')\nplt.legend(fontsize=16)\nplt.xlabel('$x_1$', fontsize=16)\nplt.ylabel('$x_2$', fontsize=16);\n\n\n\n\n\n\n\n\nThe second model definitely generalizes better.\nExample 6-3\nConsider a regression problem with decision trees where the data are generated from sklearn.datasets.make_regression.\n\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\n\n# Generate some sample data\nX, y = make_regression(n_samples=200, n_features=1, noise=2.0, random_state=86)\n\n# Visualize the data\n\nplt.scatter(X[:,0], y, c='b', s=16)\nplt.xlabel('$x$', fontsize=16)\nplt.ylabel('$f(x)$', fontsize=16)\nplt.title('Synthetic data for decision tree regressor');\n\n\n\n\n\n\n\n\n\n# Fit a Decision Tree Regressor\nregressor = DecisionTreeRegressor(min_samples_leaf=6)\nregressor.fit(X, y)\n\nDecisionTreeRegressor(min_samples_leaf=6)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  DecisionTreeRegressor?Documentation for DecisionTreeRegressoriFittedDecisionTreeRegressor(min_samples_leaf=6) \n\n\n\n# plot the decision tree regression model\n# generage grid\nx = np.linspace(X[:,0].min(), X[:,0].max(), 100)\nx = x[:, np.newaxis]\nyhat = regressor.predict(x)\nplt.scatter(X[:,0], y, c='b', s=16, label='data')\nplt.plot(x, yhat, 'g', label='decision tree model')\nplt.legend(fontsize=16)\nplt.xlabel('$x$', fontsize=16)\nplt.ylabel('$f(x)$', fontsize=16);",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Decision Trees</span>"
    ]
  },
  {
    "objectID": "Chapter7_EnsembleLearning.html",
    "href": "Chapter7_EnsembleLearning.html",
    "title": "3  Ensemble Learning",
    "section": "",
    "text": "3.1 Bagging (Bootstrap Aggregating)\nThe idea of bagging, short for Bootstrap Aggregating, is to train multiple models independently on different bootstrapped subsets of the training data, and then the predictions are averaged (for regressors) or voted upon (for classifiers). Bootstrapping is a statistical technique that generates multiple samples from a single (small) dataset by sampling with replacement so that estimates of the distribution of a statistic, e.g., the mean, variance, or confidence interval, can be made when the underlying distribution of the data is unknown. The following diagram illustrates how boostrapping works.\nBagging is particularly effective for reducing variance and preventing overfitting, especially in models with high variability such as decision trees. A high-variability model highly depends on the training dataset. If a different training set is used, then the model can behave quite differently. With bootstrapping, we are creating a group of approximately independent and identically distributed (i.i.d.) training sets, and an individual model (with high variance) is trained on each of the sets. By combining the models and averaging the predictions, the ensemble is likely less variable than any of its component learners.\nLet \\(\\{(\\boldsymbol{x}_1, y_1), \\dots, (\\boldsymbol{x}_N, y_N)\\}\\) be a training dataset for a regression problem. Each feature \\(\\boldsymbol{x}_i\\) is \\(d\\)-dimensional. Suppose \\(M\\) samples each of size \\(N\\) are obtained by bootstrapping. For each bootstrapped samples \\(S_1, S_2,\\dots, S_M\\), we fit a model \\(f_i(x), i=1,\\dots,M\\). Then the ensemble estimate \\(f^\\text{E}(\\boldsymbol{x})\\) for a new data instance \\(\\boldsymbol{x}\\) is computed as the average of the predictions from the individual models:\n\\[\\begin{equation*}\nf^\\text{E}(\\boldsymbol{x}) = \\frac{1}{M}\\sum_{i=1}^Mf_i(\\boldsymbol{x})\n\\end{equation*}\\]\nFor a \\(K\\)-class classification problem, the algorithm works similarly. With the individual learners \\(f_i(x), i=1,\\dots,M\\), we can obtain a vector \\((p_1(\\boldsymbol{x}), p_2(\\boldsymbol{x}), \\dots, p_K(\\boldsymbol{x}))\\), where \\(p_i(\\boldsymbol{x})\\) represents the proportion of the learners that predict class \\(i\\) for the new instance \\(\\boldsymbol{x}\\). Then the ensemble estimate is:\n\\[\\begin{equation*}\nf^\\text{E}(\\boldsymbol{x}) = \\text{arg}\\max_{k\\in \\{1,2,\\dots,K\\}}p_k(\\boldsymbol{x}) = \\text{arg}\\max_{k\\in \\{1,2,\\dots,K\\}}\\sum_{i=1}^M \\mathbb{I}(f_i(\\boldsymbol{x})=k)\n\\end{equation*}\\]\nwhere \\(\\mathbb{I}(\\cdot)\\) is the indicator function. This above voting method is called hard voting or majority voting. Another way of voting is to consider at the probability of predicting a class for a new instance \\((\\boldsymbol{x})\\) for each learner \\(f_i\\), if the individual learners are equipped with such probabilities (e.g. decision trees). Let \\(p_{i,j}(\\boldsymbol{x})\\) denote the probability of learner \\(f_i\\) predicting class \\(j\\) for data instance \\((\\boldsymbol{x})\\), where \\(1\\le i\\le N\\), and \\(1\\le j\\le K\\). If we average these probabilities for each \\(j\\), and find the class with the largest average, we can define the soft voting rule:\n\\[\\begin{equation*}\nf^\\text{E}(\\boldsymbol{x}) = \\text{arg}\\max_{k\\in \\{1,2,\\dots,K\\}}\\left\\{\\frac{1}{M}\\sum_{i=1}^Mp_{i,k}\\right\\}\n\\end{equation*}\\]\nFor instance, suppose there are three models \\(f_1, f_2, f_3\\) in the ensemble to predict two classes labeled \\(1\\) and \\(2\\), and the probabilities are:\n\\[\\begin{equation*}\np_{1,1} = 0.7, p_{1,2} = 0.3, p_{2,1} = 0.4, p_{1,2} = 0.6, p_{3,1} = 0.8, p_{3,2} = 0.2,\n\\end{equation*}\\]\nThen the average probabilities are \\((0.7+0.4+0.8)/3=0.63\\) for predicting class 1, and \\((0.3+0.6+0.2)/3=0.37\\) for predicting class 2. The final prediction is class 1 based on the soft voting rule. Soft voting considers the confidence levels of each model, and hence can lead to more accurate ensemble prediction, especially when the individual models are not in strong agreement. However, it does require the individual learners to be capable of outputting probabilities associated with predictions, which many models fail to do.\nIn the bagging algorithm above, we assume the sampling is done with replacement. In the case of no replacement, the method is called pasting. In some cases, we may want to sample from the feature space (i.e., use a subset of features), e.g., when the dimension of the feature space is large. If both the features and data points are randomly selected to create distinct training sets for individual models, then the method is called random patches. If only the features are sampled and all the data points are used for the individual models, then the method is called random subspaces. In addition, we noticed an obvious advantage of applying bagging (or pasting, random patches, random subspaces), which is training the individual learners can be easily parallelized. This property enables us to train a ensemble model with a large number of components.\nExample 7-1\nConstruct a bagging ensemble of 50 decision trees with no regularization for the Iris dataset. Use the petal length and width as features. Each tree component is trained with 100 bootstrapped instances.\nfrom sklearn.datasets import load_iris\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Load the Iris dataset\n# Use only the last two features petal length and width\niris = load_iris()\nX, y = iris.data[:, 2:], iris.target\n\n# Creating the bagging ensemble,\n# each member with a sample size of 100.\nbag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n                            max_samples=100, random_state=32)\nbag_clf.fit(X, y)\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n                  n_estimators=50, random_state=32)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n                  n_estimators=50, random_state=32) estimator: DecisionTreeClassifierDecisionTreeClassifier()  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier()\n# Plot the decision boundary\n# generage grid\nx1 = np.linspace(X[:,0].min()-0.1, X[:,0].max()+0.1, 100)\nx2 = np.linspace(X[:,1].min()-0.1, X[:,0].max()+0.1, 100)\nX1, X2 = np.meshgrid(x1, x2)\n# flatten X1 and X2\nr1, r2 = X1.flatten(), X2.flatten()\n# make r1 and r2 2D\nr1, r2 = r1.reshape((len(r1), 1)), r2.reshape((len(r2), 1))\n# horizontally stack r1 and r2\ngrid = np.hstack((r1,r2))\n# now grid is a feature matrix\n# get predicted labels for grid\nyhat = bag_clf.predict(grid)\n# reshape yhat so that it has the same shape as X1 and X2\nZZ = yhat.reshape(X1.shape)\nplt.contourf(X1, X2, ZZ, cmap='Paired')\nplt.scatter(X[y == 0, 0], X[y == 0, 1],\nmarker='o', c='b', s=24, label='Iris-Setosa')\nplt.scatter(X[y == 1, 0], X[y == 1, 1],\nmarker='s', c='g', s=24, label='Iris-Versicolor')\nplt.scatter(X[y == 2, 0], X[y == 2, 1],\nmarker='s', c='y', s=24, label='Iris-Virginica')\nplt.legend(fontsize=16)\nplt.xlabel('Petal length', fontsize=16)\nplt.ylabel('Petal width', fontsize=16);\nplt.title('Decision boundary for the bagging ensemble');",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "Chapter7_EnsembleLearning.html#bagging-bootstrap-aggregating",
    "href": "Chapter7_EnsembleLearning.html#bagging-bootstrap-aggregating",
    "title": "3  Ensemble Learning",
    "section": "",
    "text": "3.1.1 Out-of-Bag Score\nA byproduct of bagging is that we have a measure to estimate how well the ensemble model performs for new data points, without actually having new data points or evaluating the ensemble model. The reason is that each individual model only sees part of the training data points, since the training set for each individual model is obtained from bootstrapping the entire training dataset. Hence, how the models perform on the data instances they did not see during the training process can be an estimator on how well the ensemble model generalizes. To be specific, a measure can be defined in this way: 1) for each data instance in the training set, we find all the models that did not use it during the training process; 2) evaluate these models at the data instance, and take the majority vote; 3) the majority vote is either equal to the true label or not; we compute the proportion of the data instances for which the true labels equal the majority votes. The proportion is defined as the out-of-bag score (OOB score). Mathematically, let \\(D_i\\), \\(1\\le i\\le M\\) be the set of data points used to train model \\(f_i\\). Denote the out-of-bag majority vote for an instance \\(\\boldsymbol{x}\\) as \\(f^{\\text{E}}_{\\text{OOB}}(\\boldsymbol{x}\\). Then\n\\[\\begin{equation*}\nf^{\\text{E}}_{\\text{OOB}}(\\boldsymbol{x}) = \\text{arg}\\max_{k\\in\\{1,2,\\dots,K\\}}\\sum_{i=1}^M\\mathbb{I}(f_i(\\boldsymbol{x})=k)\\cdot \\mathbb{I}(\\boldsymbol{x}\\notin D_i)\n\\end{equation*}\\]\nand the OOB score can be computed as\n\\[\\begin{equation*}\ns_{\\text{OOB}} = \\frac{1}{N} \\sum_{i=1}^N\\mathbb{I}(f^{\\text{E}}_{\\text{OOB}}(\\boldsymbol{x}_i)=y_i)\n\\end{equation*}\\]\nSimilary, we can define the OOB error, which is simply \\(1\\) minus the OOB score:\n\\[\\begin{equation*}\ne_{\\text{OOB}} = \\frac{1}{N} \\sum_{i=1}^N\\mathbb{I}(f^{\\text{E}}_{\\text{OOB}}(\\boldsymbol{x}_i)\\ne y_i) = 1- s_{\\text{OOB}}\n\\end{equation*}\\]\nWe now continue with the previous example and compute the OOB score.\n\n# The difference is to set oob_score=True\nbag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50,\n                            max_samples=100, oob_score=True, random_state=32)\nbag_clf.fit(X, y)\n\nBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n                  n_estimators=50, oob_score=True, random_state=32)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.  BaggingClassifier?Documentation for BaggingClassifieriFittedBaggingClassifier(estimator=DecisionTreeClassifier(), max_samples=100,\n                  n_estimators=50, oob_score=True, random_state=32) estimator: DecisionTreeClassifierDecisionTreeClassifier()  DecisionTreeClassifier?Documentation for DecisionTreeClassifierDecisionTreeClassifier() \n\n\n\n# Now we can see what the OOB socre is:\nbag_clf.oob_score_\n\n0.96\n\n\nTo see the detailed information on \\(f^{\\text{E}}_{\\text{OOB}}\\) for each data instance \\(\\boldsymbol{x}\\), we can do\n\nbag_clf.oob_decision_function_\n\narray([[1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [1.        , 0.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.79310345, 0.20689655],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.93181818, 0.06818182],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.07692308, 0.92307692],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.71428571, 0.28571429],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.93333333, 0.06666667],\n       [0.        , 0.        , 1.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.125     , 0.875     ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.9375    , 0.0625    ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 1.        , 0.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.03448276, 0.96551724],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.00925926, 0.99074074],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.76984127, 0.23015873],\n       [0.        , 0.00961538, 0.99038462],\n       [0.        , 0.00961538, 0.99038462],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.05263158, 0.94736842],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.04761905, 0.95238095],\n       [0.        , 0.03225806, 0.96774194],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.00833333, 0.99166667],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.96      , 0.04      ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.15909091, 0.84090909],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.1484375 , 0.8515625 ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.01      , 0.99      ],\n       [0.        , 0.50438596, 0.49561404],\n       [0.        , 0.15833333, 0.84166667],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.375     , 0.625     ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.46153846, 0.53846154],\n       [0.        , 0.04      , 0.96      ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.01190476, 0.98809524],\n       [0.        , 0.3452381 , 0.6547619 ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.04347826, 0.95652174],\n       [0.        , 0.04545455, 0.95454545],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.03846154, 0.96153846],\n       [0.        , 0.04166667, 0.95833333],\n       [0.        , 0.03125   , 0.96875   ],\n       [0.        , 0.        , 1.        ],\n       [0.        , 0.09782609, 0.90217391]])\n\n\nFor example, for the last data instance, among all the individual models not using it during the training stage, 90.2% of them predict it to be Verginica, while 9.8% predict it to be Versicolor (\\(f^{\\text{E}}_{\\text{OOB}}=3\\)).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "Chapter7_EnsembleLearning.html#random-forests",
    "href": "Chapter7_EnsembleLearning.html#random-forests",
    "title": "3  Ensemble Learning",
    "section": "3.2 Random Forests",
    "text": "3.2 Random Forests\nRandom Forest (RF) is a variation of bagging. The motivation behind RF is to create training datasets for individual trees that are less dependent on each other (a collection of decorrelated trees). To this end, more randomness is introduced in the sampling process. For each individual model, a bootstrapped sample is first randomly selected. In the following process that construct a decision tree, for each node, instead of looking at all the possible choices of feature for a split and all the split points, RF randomly selects a subset of \\(k\\) features, where \\(k&lt;d\\), a split criterion is decided based on the subset of features. Note that if \\(k=d\\), then the ensemble is simply a regular bagging of decision trees. Usually, \\(k\\) is chosen as \\(\\log_2 d\\). The randomness resulting from randomly sampling the features increase the independency of the individual models in the ensemble.\nNow we use RandomForestClassifier to perform RF with Python for the wine data set (https://scikit-learn.org/stable/datasets/toy_dataset.html#wine-dataset).\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Load the Wine dataset\nwine = load_wine()\nX, y = wine.data, wine.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize and train the Random Forest Classifier\nrf_clf = RandomForestClassifier(n_estimators=100, random_state=21)\nrf_clf.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = rf_clf.predict(X_test)\n\n# Evaluate the model's performance\naccuracy = rf_clf.score(X_test, y_test)\nprint(f\"Random Forest Classifier Accuracy: {accuracy:.2f}\")\n\nRandom Forest Classifier Accuracy: 1.00\n\n\nWe achieved an accuracy of \\(1.0\\). It is not surprising that random forest performs so well. Actually, RF is used much more often than the other bagging techniques, and it is also commonly used as a baseline model, before more complicated models, such as deep neural networks, are attempted.\n\n3.2.1 Importance Score associate with Random Forests\nIn Random Forests, feature importance scores are a byproduct that can be used to evaluate the significance of each feature in predicting the target variable. Feature importance is an important topic in machine learning interpretability/explainability. Many models do not carry built-in feature importance scores as RF, and hence one has to apply some model-agnostic methods to compute those scores. As a result, this is a key advantage of Random Forests, as they directly provide a way to understand which features are most influential in the model’s predictions.\nThe most common method for calculating feature importance for RF is by considering the how much each feature contributes to reducing the impurity (e.g., Gini impurity) in the decision trees within the Random Forest. For each feature, the decrease in impurity is averaged over all the trees in the forest. A feature that results in a significant decrease in impurity is considered more important.\n\n# Print out the feature importances for all features:\nprint('feature importance:', rf_clf.feature_importances_)\nprint()\n# A better print-out: including the feature names,\n# so that we know which feature importance is for which feature\nfor i in range(rf_clf.feature_importances_.size):\n    print(wine[\"feature_names\"][i], rf_clf.feature_importances_[i])\n\nfeature importance: [0.12920959 0.02570678 0.01617244 0.02701698 0.02524581 0.06519636\n 0.15131565 0.01371709 0.0223001  0.17238892 0.07560528 0.14544402\n 0.13068099]\n\nalcohol 0.1292095910497735\nmalic_acid 0.025706776014575165\nash 0.016172443959041227\nalcalinity_of_ash 0.027016975258495418\nmagnesium 0.025245808545271965\ntotal_phenols 0.06519636188629413\nflavanoids 0.1513156473223214\nnonflavanoid_phenols 0.013717093062667347\nproanthocyanins 0.022300097134065243\ncolor_intensity 0.17238891683084545\nhue 0.07560527634220683\nod280/od315_of_diluted_wines 0.1454440242300447\nproline 0.1306809883643975\n\n\nThe results show that the features “alcohol”, “flavanoids”, “color_intensity”, “od280/od315_of_diluted_wines”, and “proline” play a more important role than the others in the predictions of the ensemble model.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "Chapter7_EnsembleLearning.html#boosting",
    "href": "Chapter7_EnsembleLearning.html#boosting",
    "title": "3  Ensemble Learning",
    "section": "3.3 Boosting",
    "text": "3.3 Boosting\nBoosting works differently from bagging, where a collection of independent weak learners with high variance are combined to produce a model that generalizes well. For boosting, the idea is that a collection of sequential models with high bias are combined to produce a stronger predictive model. Each new model in the ensemble is trained sequentially with the purpose of correcting the errors made by the previous models. Two boosting techniques are commonly used: AdaBoost and Gradient Boosting. We start with the discussion of AdaBoost.\n\n3.3.1 Adaptive Boosting (AdaBoost)\nThe idea of AdaBoost is to assign weights to training data instances. A subsequent model tries to put more weights on the instances that are predicted wrong by the previous models. The final ensemble model is a linear combination (weighted sum) of all the individual models with more accurate individual models assigned a larger coefficients (weights). The algorithm works as follows.\nStep 1. Initialize the weights for data instances\n\nAll data instances are initially assigned an equal weight, \\(w_i^{(1)} = \\frac{1}{N}\\), \\(1\\le i\\le N\\). Here the superscript denotes the iteration number.\n\nStep 2. Train weak learners\nFor each iteration \\(m\\), \\(1\\le i\\le M\\), where \\(M\\) is the number of individual models to be constructed,\n\ntrain a weak learner \\(f_m\\), such as a shallow decision tree, based on the weighted samples. That is, minimizing a weighted error function (e.g., in Scikit-Learn the fit method for the classifier has a sample_weight optional input).\ncompute the weighted error:\n\n\\[\\begin{equation*}\n\\epsilon_m = \\frac{\\sum_{i=1}^{N} w_i^{(m)} \\cdot \\mathbb{I}\\{ f_m(\\boldsymbol{x}_i) \\neq y_i \\}}{\\sum_{i=1}^{N} w_i^{(m)}}\n\\end{equation*}\\] This can be explained as the weighted sum of misclassified instances. The denominator is always 1 as seen in the following steps. Note that if the weights are equal for all data instances, \\(\\epsilon_m\\) is simply the proportion of data instances that are predicted wrong by \\(f_m\\).\n\ncompute the weight of learner \\(f_m\\)\n\n\\[\\begin{equation*}\n\\alpha_m = \\frac{1}{2} \\ln \\left(\\frac{1 - \\epsilon_m}{\\epsilon_m}\\right)\n\\end{equation*}\\]\nThe smaller \\(\\epsilon_m\\), the larger the weight \\(\\alpha_m\\), as seen below. As \\(\\alpha_m\\) approaches \\(0.5\\) (meaning approaching a random model), \\(\\alpha_m\\) is close to 0.\n\neps = np.linspace(0.00001, 0.5, 100)\nplt.plot(eps, np.log((1-eps)/eps), 'b-')\nplt.xlabel('$\\epsilon_m$')\nplt.ylabel('$\\ln{((1-\\epsilon_m)/\\epsilon_m)}$');\n\n\n\n\n\n\n\n\nStep 3. Update Sample Weights\n\nIncrease the weights of \\(f_m\\)-misclassified samples:\n\n\\[\\begin{equation*}\nw_i^{(m+1)} = w_i^{(m)} \\cdot \\exp \\left( \\alpha_m \\cdot \\mathbb{I}\\{ f_m(\\boldsymbol{x}_i) \\neq y_i \\} \\right),\\quad 1\\le i\\le N\n\\end{equation*}\\] This means the weights of the misclassified instances are magnified, while the weights of the correctly classified instances get smaller, due to the normalization below.\n\nNormalize weights\n\n\\[\\begin{equation*}\nw_i^{(m+1)} = \\frac{w_i^{(m+1)}}{\\sum_{j=1}^{N} w_j^{(m+1)}},\\quad 1\\le i\\le N\n\\end{equation*}\\]\nStep 4. Combine Weak Learners\n\nThe final ensemble model is:\n\n\\[\\begin{equation*}\nf^{\\text{E}}(\\boldsymbol{x}) = \\text{sign} \\left( \\sum_{m=1}^{M} \\alpha_m \\cdot f_m(\\boldsymbol{x}) \\right)\n\\end{equation*}\\]\nThe principles behind these formulas are the minimization of an exponential loss function (see Friedman (2000)). Due to the complexity of the derivation, we will leave it out.\nExample 7-2\nApply AdaBoost to the breast cancer dataset (https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset). The weak learners are decision trees with a maximum depth of 1.\n\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\n# Load the Breast Cancer dataset\ndata = load_breast_cancer()\nX, y = data.data, data.target\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Initialize and train the weak learners\nada_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), \n                               algorithm='SAMME',  # for the purpose of suppressing a warning\n                               n_estimators=100, random_state=90)\nada_model.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = ada_model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"AdaBoost Classifier Accuracy: {accuracy:.2f}\")\n\nAdaBoost Classifier Accuracy: 0.97\n\n\n\n\n3.3.2 Gradient Boosting\nGradient Boosting builds models sequentially in the way that each subsequent model is trained to correct the residual errors of the combined predictions from previous models. The final model is a weighted sum of all the individual models. The name comes from the fact that it uses gradient descent to minimize the loss function. Here are the details of the algorithm.\nStep 1. Initialize a model\nThe initial model, \\(f_0(\\boldsymbol{x})\\), used in gradient boosting is typically a constant function, and the lost function \\(L\\) is typically the Mean Squared Error (MSE). Hence the initial model is:\n\\[\\begin{equation*}\nf_0(\\boldsymbol{x}) = \\arg \\min_c \\sum_{i=1}^{N} L(y_i, c)\n\\end{equation*}\\]\nWe have seen in the chapter on decision trees that the solution to the optimization problem is trivial:\n\\[\\begin{equation*}\nf_0(\\boldsymbol{x}) =\\frac{1}{N}\\sum_{i=1}^{N} y_i\n\\end{equation*}\\]\nStep 2. Compute the residuals\nFor each iteration \\(m\\), \\(1\\le i\\le M\\), where \\(M\\) is the number of individual models to be constructed, we are trying to find the \\(m\\)th individual model \\(f_m\\). By the motivation of gradient boosting, \\(f_m\\) approximates the residuals of the current model, which is the negative gradient of the loss function \\(L\\) with respect to the current prediction:\n\\[\\begin{equation*}\nr_i^{(m)} = -\\left[\\frac{\\partial L(y_i, F(\\boldsymbol{x}_i))}{\\partial F(\\boldsymbol{x}_i)}\\right]_{F(\\boldsymbol{x})=F_{m-1}(\\boldsymbol{x})}\n\\end{equation*}\\]\nwhere \\(F_{m-1}(\\boldsymbol{x})\\) is the ensemble model with \\(m-1\\) individual models. The residual represents the direction where the ensemble model needs to move to fast decrease the cost function. In the case of \\(L\\) being the MSE function, the residual \\(r_i^{(m)}\\) is simply \\(y_i-F_{m-1}(\\boldsymbol{x_i})\\), the difference between the true value (label) and the current ensemble prediction.\nStep 3. Fit the new individual model \\(f_m\\) that approximates the residual \\(r_i^{(m)}\\)\n\\[\\begin{equation*}\nf_m(\\boldsymbol{x}) = \\arg \\min_{f} \\sum_{i=1}^{N} \\left(r_i^{(m)} - f(\\boldsymbol{x}_i)\\right)^2\n\\end{equation*}\\]\nStep 4. Update the ensemble model \\(F_m(\\boldsymbol{x})\\)\n\\[\\begin{equation*}\nF_m(\\boldsymbol{x}) = F_{m-1}(\\boldsymbol{x}) + \\eta f_m(\\boldsymbol{x})\n\\end{equation*}\\]\nwhere \\(\\eta\\) is the learning rate.\nAt the end, we have the final gradient boost ensemble model, \\(F^{\\text{E}}(\\boldsymbol{x})=F_M(\\boldsymbol{x})\\). It is clearly seen that the whole process is a gradient descent in the function space.\nNow we use decision trees as the individual models in the gradient boosting ensemble for a synthetic problem. Such gradient boosting is called Gradient Tree Boosting, or Gradient Boosted Regression Trees (GBRT).\nExample 7-3\nUse sklearn.ensemble.GradientBoostingRegressor to build a gradient boosting model for noisy data generated from the underlying function\n\\[\\begin{equation*}\nf(x) = 2x + \\sin{x}\n\\end{equation*}\\]\n\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\n\n# Create a synthetic dataset\nndata = 100  # 100 data points\nnp.random.seed(10)\nX = np.linspace(0, 2*np.pi, ndata)  # data points are between 0 and 2pi\ny = 2 * X + np.sin(X) + np.random.normal(0, 0.5, X.size)\nX = X[:, np.newaxis]\n\n# Initialize and train a Gradient Boosting Regressor with 3 components.\ngbr3 = GradientBoostingRegressor(n_estimators=3, max_depth=1, learning_rate=1, random_state=42)\ngbr3.fit(X, y)\n\n# Predictions\ny_pred_3 = gbr3.predict(X)\n\n# Plotting the results\nfig, ax = plt.subplots(1, 3, figsize=(21, 6))\n\nax[0].plot(X, y, 'bo', label='Training Data')\nax[0].plot(X, y_pred_3, 'g-', label='GBRT-M=3')\nax[0].legend()\nax[0].set_xlabel('x')\nax[0].set_ylabel('y');\n\n# Initialize and train a Gradient Boosting Regressor with 10 components.\ngbr10 = GradientBoostingRegressor(n_estimators=10, max_depth=1, learning_rate=1, random_state=42)\ngbr10.fit(X, y)\n\n# Predictions\ny_pred_10 = gbr10.predict(X)\n\n# Plotting the results\nax[1].plot(X, y, 'bo', label='Training Data')\nax[1].plot(X, y_pred_10, 'g-', label='GBRT-M=10')\nax[1].legend()\nax[1].set_xlabel('x')\nax[1].set_ylabel('y');\n\n# Initialize and train a Gradient Boosting Regressor with 20 components.\ngbr20 = GradientBoostingRegressor(n_estimators=20, max_depth=1, learning_rate=1, random_state=42)\ngbr20.fit(X, y)\n\n# Predictions\ny_pred_20 = gbr20.predict(X)\n\n# Plotting the results\nax[2].plot(X, y, 'bo', label='Training Data')\nax[2].plot(X, y_pred_20, 'g-', label='GBRT-M=20')\nax[2].legend()\nax[2].set_xlabel('x')\nax[2].set_ylabel('y');\n\n\n\n\n\n\n\n\nThere is an obvious improvement by increasing \\(M\\) from 3 to 10. Further increasing \\(M\\) to 20 does not lead to obvious change of the ensemble model. The gradient boosting algorithm tells us that the ensemble model is obtained iteratively by adding the component that approximates the previous residuals. We verify this by not using the built-in GradientBoostingRegressor class.\n\nfrom sklearn.tree import DecisionTreeRegressor\n\nfig, ax = plt.subplots(3, 2, figsize=(14, 21))\n\n# Train the DT regressor on X and y\n# It is the first decision tree regressor on the data\ndt_reg1 = DecisionTreeRegressor(max_depth=1)\ndt_reg1.fit(X, y)\n\nax[0,0].plot(X, y, 'bo', label='Training Data')\nax[0,0].plot(X, dt_reg1.predict(X), 'g-', label='$f_1(x)$')\nax[0,0].legend()\nax[0,0].set_xlabel('x')\nax[0,0].set_ylabel('y')\nax[0,0].set_title('Tree predictions, M=1')\n\n# The ensemble will have only one individual model\nax[0,1].plot(X, y, 'bo', label='Training Data')\nax[0,1].plot(X, dt_reg1.predict(X), 'r-', label='$F(x) = f_1(x)$')\nax[0,1].legend()\nax[0,1].set_xlabel('x')\nax[0,1].set_ylabel('y');\nax[0,1].set_title('Ensemble predictions');\n\n\n# Train the second DT regressor on the residual errors made by the first predictor\n# It is the second decision tree regressor on the data\nr = y - dt_reg1.predict(X)\ndt_reg2 = DecisionTreeRegressor(max_depth=1)\ndt_reg2.fit(X, r)\n\nax[1,0].plot(X, r, 'bo', label='Training Data')\nax[1,0].plot(X, dt_reg2.predict(X), 'g-', label='$f_2(x)$')\nax[1,0].legend()\nax[1,0].set_xlabel('x')\nax[1,0].set_ylabel('y')\nax[1,0].set_title('Tree predictions, M=2')\n\n# The ensemble will have two individual models\nax[1,1].plot(X, y, 'bo', label='Training Data')\nax[1,1].plot(X, dt_reg1.predict(X)+dt_reg2.predict(X), 'r-', label='$F(x) = f_1(x)+f_2(x)$')\nax[1,1].legend()\nax[1,1].set_xlabel('x')\nax[1,1].set_ylabel('y');\nax[1,1].set_title('Ensemble predictions');\n\n# Train the third DT regressor on the residual errors made by the first two predictors\n# It is the third decision tree regressor on the data\nr = r - dt_reg2.predict(X)\ndt_reg3 = DecisionTreeRegressor(max_depth=1)\ndt_reg3.fit(X, r)\n\nax[2,0].plot(X, r, 'bo', label='Training Data')\nax[2,0].plot(X, dt_reg3.predict(X), 'g-', label='$f_3(x)$')\nax[2,0].legend()\nax[2,0].set_xlabel('x')\nax[2,0].set_ylabel('y')\nax[2,0].set_title('Tree predictions, M=3')\n\n# The ensemble will have two individual models\nax[2,1].plot(X, y, 'bo', label='Training Data')\nax[2,1].plot(X, dt_reg1.predict(X)+dt_reg2.predict(X)+dt_reg3.predict(X), \n             'r-', label='$F(x) = f_1(x)+f_2(x)+f_3(x)$')\nax[2,1].legend()\nax[2,1].set_xlabel('x')\nax[2,1].set_ylabel('y');\nax[2,1].set_title('Ensemble predictions');\n\n\n\n\n\n\n\n\nNote that the plot in the bottom right corner is the same as that for the gradient boosting regressor with 3 components trained using the built-in class in the previous cell, confirming that either way we obtain the same ensemble model.\n\n\n3.3.3 Optimal Ensemble Size\nWe can use a validation set to monitor the model’s performance and perform early stopping when the model’s performance on the validation set starts to decrease. The Python code below does the job.\n\nfrom sklearn.metrics import mean_squared_error\n\n# We use the same synthetic data\n\n# Split the data into training and validation sets\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=30)\n\n# Initialize a Gradient Boosting Regressor with a large number of estimators\ngbr = GradientBoostingRegressor(n_estimators=200, learning_rate=1, max_depth=1, random_state=42)\n\n# Fit the model while tracking the validation error at each stage\ngbr.fit(X_train, y_train)\n\n# Compute validation error after each stage\nerrors_val = [mean_squared_error(y_val, y_pred) for y_pred in gbr.staged_predict(X_val)]\n# Compute training error after each stage\nerrors_train = [mean_squared_error(y_train, y_pred_train) for y_pred_train in gbr.staged_predict(X_train)]\n\n# Find the optimal number of estimators (minimizing validation error)\nbest_n_estimators = np.argmin(errors_val)  \nprint(f\"Optimal number of estimators: {best_n_estimators+1}\")  # Adding 1 because indexing starts at 0\n\n# Plot the validation error\nplt.figure(figsize=(10, 6))\nplt.plot(range(1, len(errors_val)+1), errors_val, label='Validation Error')\nplt.plot(range(1, len(errors_train)+1), errors_train, label='Training Error')\nplt.axvline(best_n_estimators+1, color='red', linestyle='--', label=f'Optimal # of Estimators = {best_n_estimators+1}')\nplt.xlabel('Number of Estimators')\nplt.ylabel('Validation Error')\nplt.title('Finding Optimal Number of Estimators')\nplt.legend()\nplt.show()\n\n# Re-train the model with the optimal number of estimators\ngbr_optimal = GradientBoostingRegressor(n_estimators=best_n_estimators, learning_rate=1, max_depth=1, random_state=15)\ngbr_optimal.fit(X_train, y_train)\n\n# Evaluate on the validation set\ny_val_pred = gbr_optimal.predict(X_val)\nmse_val = mean_squared_error(y_val, y_val_pred)\nprint(f\"Validation MSE with optimal estimators: {mse_val:.2f}\")\n\nOptimal number of estimators: 39\n\n\n\n\n\n\n\n\n\nValidation MSE with optimal estimators: 0.62",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "Chapter7_EnsembleLearning.html#references",
    "href": "Chapter7_EnsembleLearning.html#references",
    "title": "3  Ensemble Learning",
    "section": "3.4 References",
    "text": "3.4 References\n\nJ. Friedman, T. Hastie, and R. Tibshirani, Additive logistic regression: a statistical view of boosting (With discussion and a rejoinder by the authors), Annals of Statistics, 28(2): 337-407.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Ensemble Learning</span>"
    ]
  },
  {
    "objectID": "Chapter8_DimensionReduction.html",
    "href": "Chapter8_DimensionReduction.html",
    "title": "4  Dimension Reduction",
    "section": "",
    "text": "4.1 Principal Component Analysis (PCA)\nPrincipal component analysis (PCA) transforms data of a higher dimension to a new coordinate system with a lower dimension. During the transformation, the variance of the data, which can be considered as the information the data contain, is conserved as much as possible. The core of the algorithm is an eigendecomposition of the covariance matrix constructed with the mean-removed/centered data, leading to a set of eigenvectors, called principal components, and eigenvalues, which provides important information on the dimension of the reduced coordinates.\nSuppose we have a set of data points/observations \\(\\{\\boldsymbol{x}_1, \\boldsymbol{x}_2, \\dots, \\boldsymbol{x}_N\\}\\), and each data point is \\(d\\)-dimensional, i.e., \\(\\boldsymbol{x}_i=(x_i^1, x_i^2, \\dots, x_i^d)^T\\). Combining all the data points into a data matrix \\(X\\),\n\\[\\begin{equation*}\nX =\n\\begin{bmatrix}\nx_1^1 & x_1^2 & \\dots & x_1^d \\\\\nx_2^1 & x_2^2 & \\dots & x_2^d \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nx_N^1 & x_N^2 & \\dots & x_N^d\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\boldsymbol{x}_1^T \\\\\n\\boldsymbol{x}_2^T \\\\\n\\vdots \\\\\n\\boldsymbol{x}_N^T\n\\end{bmatrix}\n\\end{equation*}\\]\nCompute the mean of each feature, \\(\\bar{x}^i = \\frac{1}{N}\\sum_{j=1}^Nx_{j}^i\\), \\(1\\le i\\le d\\). Subtracting the mean feature from each feature in the data matrix \\(X\\), i.e. each \\(x_{i}^j\\) element in \\(X\\) is replaced by \\(x_i^j-\\bar{x}^j\\), we obtain the centered data matrix \\(X_c\\). The covariance matrix of \\(X_c\\) can be computed as\n\\[\\begin{equation*}\nC = \\frac{1}{N}X_c^T X_c\n\\end{equation*}\\]\nPerforming an eigenvalue decomposition on the covariance matrix, we obtain a set of eigenvalues \\(\\{\\lambda_1,\\dots,\\lambda_d\\}\\), and a set of \\(d\\)-dimensional normalized (unit) eigenvectors \\(\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2, \\dots, \\boldsymbol{u}_d\\}\\). The eigenvectors are called principal components. These are the directions in the \\(d\\)-dimensional space the original data points \\(\\boldsymbol{x}_i\\)’s will be projected to in order to get a new set of coordinates. Supposing we only keep a small set of the eigenvalues, \\(\\{\\lambda_1,\\dots,\\lambda_m\\}\\), and eigenvectors, \\(\\{\\boldsymbol{u}_1, \\boldsymbol{u}_2, \\dots, \\boldsymbol{u}_m\\}\\), \\(m\\ll d\\), then the projected data will have a much smaller dimension \\(m\\). Let \\(X_r\\) be the dimension-reduced data. Then\n\\[\\begin{equation*}\nX =\n\\begin{bmatrix}\n\\boldsymbol{x}_1^T\\boldsymbol{u}_1 & \\boldsymbol{x}_1^T\\boldsymbol{u}_2 & \\dots & \\boldsymbol{x}_1^T\\boldsymbol{u}_m \\\\\n\\boldsymbol{x}_2^T\\boldsymbol{u}_1 & \\boldsymbol{x}_2^T\\boldsymbol{u}_2 & \\dots & \\boldsymbol{x}_2^T\\boldsymbol{u}_m \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\n\\boldsymbol{x}_N^T\\boldsymbol{u}_1 & \\boldsymbol{x}_N^T\\boldsymbol{u}_2 & \\dots & \\boldsymbol{x}_N^T\\boldsymbol{u}_m \\\\\n\\end{bmatrix}\n\\end{equation*}\\]\nNow we will see why the algorithm works, i.e. why the algorithm guarantees that the maximum variance is retained for any chosen \\(m\\), \\(1\\le m\\le d\\).\nLet \\(\\boldsymbol{v}_1\\) be a unit vector in the \\(d\\)-dimensional space, and it is the vector such that when all the data points are projected to it, the resulting 1-dimensional transformed data points has the largest variance. We want to show \\(\\boldsymbol{v}_1=\\boldsymbol{u}_1\\), the normalized eigenvector of \\(C\\) corresponding to the largest eigenvalue. To see this, we can compute the mean of the transformed (reduced) data points by:\n\\[\\begin{equation*}\n\\frac{1}{N}\\sum_{i=1}^N \\boldsymbol{v}_1^T\\boldsymbol{x}_i = \\boldsymbol{v}_1^T\\frac{1}{N}\\sum_{i=1}^N \\boldsymbol{x}_i = \\boldsymbol{v}_1^T\\bar{\\boldsymbol{x}}\n\\end{equation*}\\]\nwhere \\(\\bar{\\boldsymbol{x}}\\) is the mean vector of all the data points (row mean of data matrix \\(X\\)). Based on the mean, we can compute the variance of the reduced coordinates as (see Exercise 8-1):\n\\[\\begin{equation*}\n\\frac{1}{N}\\sum_{i=1}^N (\\boldsymbol{v}_1^T\\boldsymbol{x}_i - \\boldsymbol{v}_1^T\\bar{\\boldsymbol{x}})^2 = \\boldsymbol{v}_1^TC\\boldsymbol{v}_1\n\\end{equation*}\\]\nwhere \\(C\\) is the data covariance matrix defined before\n\\[\\begin{equation*}\nC = \\frac{1}{N}X_c^T X_c = \\frac{1}{N}\\sum_{i=1}^N (\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}}) (\\boldsymbol{x}_i-\\bar{\\boldsymbol{x}})^T\n\\end{equation*}\\]\nOur goal is to maximize \\(\\boldsymbol{v}_1^TC\\boldsymbol{v}_1\\), with the constraint that \\(\\boldsymbol{v}_1^T\\boldsymbol{v}_1=1\\). Using Lagrange multiplier, we can obtain the following equivalent problem:\n\\[\\begin{equation*}\n\\text{arg}\\max_{\\boldsymbol{v}_1} \\,\\,\\boldsymbol{v}_1^TC\\boldsymbol{v}_1 + \\lambda (1-\\boldsymbol{v}_1^T\\boldsymbol{v}_1)\n\\end{equation*}\\]\nTaking derivative with respect to \\(\\boldsymbol{v}_1\\) and setting it to \\(0\\) lead to\n\\[\\begin{equation*}\nC\\boldsymbol{v}_1 = \\lambda \\boldsymbol{v}_1,\n\\end{equation*}\\]\nwhich shows that \\(\\boldsymbol{v}_1\\) is an eigenvector of \\(C\\) corresponding to the eigenvalue \\(\\lambda\\). Left multiplying both sides of the previous equation by \\(\\boldsymbol{v}_1^T\\), we have\n\\[\\begin{equation*}\n\\boldsymbol{v}_1^TC\\boldsymbol{v}_1 = \\lambda\n\\end{equation*}\\]\nSince \\(\\boldsymbol{v}_1^TC\\boldsymbol{v}_1\\) is the largest, \\(\\lambda\\) must be equal to \\(\\lambda_1\\), the largest eigenvalue of \\(C\\). Therefore, we have \\(\\boldsymbol{v}_1=\\boldsymbol{u}_1\\), the eigenvector corresponding to the largest eigenvalue, also called the first principal component.\nUsing a mathematical induction, we can show the variance of the \\(m\\)-dimensional projected data points is the largest when we use the first \\(m\\) eigenvectors corresponding to the largest \\(m\\) eigenvalues of \\(C\\).\nExercise 8-1\nShow the variance can be computed by\n\\[\\begin{equation*}\n\\frac{1}{N}\\sum_{i=1}^N (\\boldsymbol{v}_1^T\\boldsymbol{x}_i - \\boldsymbol{v}_1^T\\bar{\\boldsymbol{x}})^2 = \\boldsymbol{v}_1^TC\\boldsymbol{v}_1\n\\end{equation*}\\]\nExample 8-1\nWe consider a simple 2D data that can be transformed to 1D without losing much variance. We use Python to transform the data and show how sklearn can be used to perform PCA.\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Synthetic 2D data that is very linear\nX = np.array([[2.5, 2.4],\n              [0.5, 0.7],\n              [2.2, 2.9],\n              [1.9, 2.2],\n              [3.1, 3.0],\n              [2.3, 2.7],\n              [2, 1.6],\n              [1, 1.1],\n              [1.5, 1.6],\n              [1.1, 0.9]])\n\n# mean removal\nX_c = X - np.mean(X, axis=0)\n\n# Perform PCA, m=1, i.e. only use the first principal component\npca = PCA(n_components=1)\nX_r = pca.fit_transform(X_c)\n\n# Explained variance, i.e. \\lambda_1/(\\lambda_1+\\lambda_2)\n# This is also the proportion of variance that is retained.\nexplained_variance = pca.explained_variance_ratio_\n\n# This is the first principal component\nprint(\"Principal Components:\\n\", pca.components_)\nprint(\"Explained Variance Ratio:\", explained_variance)\n\n# Plot original and reduced data\nplt.scatter(X_c[:, 0], X_c[:, 1], color='blue', label='Original Data')\nplt.scatter(X_r[:, 0], np.zeros(X_r.shape), color='red', label='Reduced Data')\n# Plot an arrow that shows the principal component direction\nplt.annotate('', xy=pca.components_[0], xytext=(0,0),\n             arrowprops=dict(facecolor='black', arrowstyle='-&gt;'))\nplt.text(-0.25, -0.5, '1st PC', fontsize=12, color='k')\n\nplt.legend()\nplt.show()\n\nPrincipal Components:\n [[-0.6778734  -0.73517866]]\nExplained Variance Ratio: [0.96318131]\nExample 8-2\nVisualize the iris dataset after transforming the data to 2D using PCA.\nfrom sklearn.datasets import load_iris\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data  # 4D features\ny = iris.target\n\n# Perform PCA to reduce the data from 4D to 2D\npca = PCA(n_components=2)\nX_r = pca.fit_transform(X)\n\nscatter = plt.scatter(X_r[:, 0], X_r[:, 1], c=y)\n# Create a custom legend\nhandles, labels = scatter.legend_elements(prop=\"colors\")\nlegend_labels = ['Setosa', 'Versicolor', 'Virginica']\nplt.legend(handles, legend_labels, title=\"Classes\");",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "Chapter8_DimensionReduction.html#choosing-m",
    "href": "Chapter8_DimensionReduction.html#choosing-m",
    "title": "4  Dimension Reduction",
    "section": "4.2 Choosing \\(m\\)",
    "text": "4.2 Choosing \\(m\\)\nA common way to guide the choice of \\(m\\) is to use the “cumulative explained variance ratio”. Suppose \\(\\tilde{m}\\) principal components are used (hence the reduced data has dimension \\(\\tilde{m}\\)), then the cumulative explained variance ratio is defined as the sum of the first \\(\\tilde{m}\\) largest eigenvalues divided by the sum of all the eigenvalues. This is an indicator of what percentage of variance is retained after transforming the data to a smaller dimension. For a preset threshold, say 90%, we will choose the number of components where the cumulative explained variance ratio first reaches the threshold to be \\(m\\).\nWe go back to the previous iris data set example to illustrate the process.\n\n# Still for the Iris dataset\n# Here we build a PCA without specifying the number of components\n# The covariance matrix will be 4 by 4 now\n# there will be 4 principal components, and hence 4 eigenvalues\npca = PCA()\npca.fit(X)\n# Show the eigenvalues of the covariance matrix\nprint('The eigenvalues are: ', pca.explained_variance_)\n\nThe eigenvalues are:  [4.22824171 0.24267075 0.0782095  0.02383509]\n\n\n\n# Plot the spectrum of the covariance matrix\nplt.plot(pca.explained_variance_, 'bo-')\nplt.xlabel('Principal component number')\nplt.ylabel('Eigenvalues')\nplt.title('The spectrum of the covariance matrix');\n\n\n\n\n\n\n\n\n\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nprint('Cumulative explained variance ratios are: ', cumsum)\n\nCumulative explained variance ratios are:  [0.92461872 0.97768521 0.99478782 1.        ]\n\n\n\n# Plot the cumulative explained variance ratio\nplt.plot(cumsum, 'rx-')\nplt.xlabel('Principal component number')\nplt.ylabel('Cum sum')\nplt.title('The cumulative sum of exp var ratio');\n\n\n\n\n\n\n\n\nFor example, if our threshold is chosen to be \\(95\\%\\), we would need \\(m=2\\), i.e., when the 4D data points are transformed to 2D with PCA, less than \\(5\\%\\) of the variance is lost. The following line of code will do this automatically:\n\nd = np.argmax(cumsum &gt;= 0.95) + 1\nprint('To retain at least 95% of variance, m should be ', d)\n\nTo retain at least 95% of variance, m should be  2\n\n\nExample 8-3\nWe consider an application of PCA to the MNIST dataset.\n\nfrom sklearn.datasets import fetch_openml\n\n\n# Load the MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\n# Feature matrix:\nX = mnist['data']\n# Target vector:\ny = mnist['target']\n\nprint('The shape of X is: ', X.shape)\nprint('The shape of y is: ', y.shape)\n\nThe shape of X is:  (70000, 784)\nThe shape of y is:  (70000,)\n\n\n\n# Perform PCA and determine the m that gives us at least 95% variance\npca = PCA()\npca.fit(X)\ncumsum = np.cumsum(pca.explained_variance_ratio_)\nd = np.argmax(cumsum &gt;= 0.95) + 1\nprint('To retain at least 95% of variance, m should be ', d)\n\nTo retain at least 95% of variance, m should be  154\n\n\n\nplt.figure(figsize=(12, 6))\nplt.subplot(1,2,1)\n# Plot the spectrum of the covariance matrix\nplt.plot(pca.explained_variance_, 'bo-')\nplt.xlabel('Principal component number')\nplt.ylabel('Eigenvalues')\nplt.title('The spectrum of the covariance matrix');\nplt.subplot(1,2,2)\n# Plot the cumulative sum of the explained variance ratio\nplt.plot(cumsum, 'rx-')\nplt.xlabel('Principal component number')\nplt.ylabel('Cum sum')\nplt.title('The cumulative sum of exp var ratio');\n\n\n\n\n\n\n\n\n\n# First compress the data to 154 dimensions\npca = PCA(n_components=154)\nX_reduced = pca.fit_transform(X)\n# Reconstruct the data back to 784 dimension.\nX_recovered = pca.inverse_transform(X_reduced)\nprint('Shape of the reconstructed data: ', X_recovered.shape)\n\nShape of the reconstructed data:  (70000, 784)\n\n\n\nimport matplotlib as mpl\n\n# Take an arbitray instance of the original data for plotting\nplt.subplot(1,2,1)\ndigit = X[300, :]\ndigit_image = digit.reshape(28,28)\nplt.imshow(digit_image, cmap = mpl.cm.binary);\n\n# Take an arbitray instance of the recovered data for plotting\nplt.subplot(1,2,2)\ndigit = X_recovered[300, :]\ndigit_image = digit.reshape(28,28)\nplt.imshow(digit_image, cmap = mpl.cm.binary);\n\n\n\n\n\n\n\n\nThe two figures, original and reconstructed, are very similar.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "Chapter8_DimensionReduction.html#incremental-principal-component-analysis-ipca",
    "href": "Chapter8_DimensionReduction.html#incremental-principal-component-analysis-ipca",
    "title": "4  Dimension Reduction",
    "section": "4.3 Incremental Principal Component Analysis (IPCA)",
    "text": "4.3 Incremental Principal Component Analysis (IPCA)\nWhen a dataset is large, PCA can be very inefficient, due to the fact that the algorithm requires that all data have to be used at once. In some cases, data come in batches, and we would like the algorithm can handle incremental data, instead of repeating the training on all the data that are currently available. Incremental Principal Component Analysis (IPCA) is a variant of PCA that can handle data in smaller chunks. To achieve this, IPCA iteratively updates the data mean, covariance matrix, and the eigenvalues and eigenvectors through an incremental singular value decomposition (SVD) procedure, as new data arrive.\nWe use the following example to illustrate how IPCA can be implemented in Python\nExample 8-4\nRevisit the MNIST data using IPCA.\n\nfrom sklearn.decomposition import IncrementalPCA\n\n# Load the MNIST dataset\nmnist = fetch_openml('mnist_784', version=1, as_frame=False)\n# Feature matrix:\nX = mnist['data']\n\n# Define the number of principal components\nn_components = 50\n\n# Initialize IncrementalPCA\nipca = IncrementalPCA(n_components=n_components)\n\n# Simulate batch processing\nbatch_size = 1000\nn_batches = X.shape[0] // batch_size\n\n# Process data in batches\nfor batch_idx in range(n_batches):\n    X_batch = X[batch_idx * batch_size:(batch_idx + 1) * batch_size]\n    ipca.partial_fit(X_batch)\n\n# Transform the data using the fitted IncrementalPCA\nX_ipca = ipca.transform(X)\n\n# Print the cumulative explained variance ratio\nprint(f'Cumulative explained variance ratio by component: {np.cumsum(ipca.explained_variance_ratio_)}')\n\nCumulative explained variance ratio by component: [0.09746102 0.16901519 0.23051014 0.28454351 0.33343241 0.37648426\n 0.40926634 0.43816185 0.46574444 0.48916443 0.51023025 0.53060464\n 0.54767341 0.56461214 0.58044427 0.59530583 0.60849752 0.62128546\n 0.63315565 0.64468204 0.65533938 0.66543267 0.67501973 0.68411242\n 0.69294025 0.70132605 0.70942184 0.7172739  0.72466989 0.73156274\n 0.73811856 0.74456287 0.75056215 0.75641254 0.76206722 0.76749188\n 0.77251804 0.77737117 0.78214475 0.78680419 0.79133058 0.79574818\n 0.79990946 0.80380231 0.80756828 0.81128911 0.81484134 0.81824902\n 0.821381   0.824395  ]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "Chapter8_DimensionReduction.html#kernel-principal-component-analysis-kpca",
    "href": "Chapter8_DimensionReduction.html#kernel-principal-component-analysis-kpca",
    "title": "4  Dimension Reduction",
    "section": "4.4 Kernel Principal Component Analysis (KPCA)",
    "text": "4.4 Kernel Principal Component Analysis (KPCA)\nPCA is effective for data whose features show a linear relationship. For a high-dimensional non-linear feature space (think about spirals or concentric circles), PCA may not perform well (\\(m\\) could be close to \\(d\\)). For non-linear feature space, we may use the idea illustrated in the discussion of kernelized SVM, where we use kernels to implicitly project linearly inseparable data to a higher-dimensional (can be infinitely dimensional) space where they become linearly separable. Similarly, we can borrow the idea and use a kernel to implicitly project a nonlinear feature space to a higher-dimensional one, which hopefully are more linear. We now introduce the main mathematics behind KPCA, which is very similar to that of PCA.\nSuppose the implicitly defined mapping from the original feature space (\\(d\\)-dimensional) to a higher-dimensional (\\(D\\)-dimensional, \\(d\\ll D\\)) one is \\(\\boldsymbol{\\phi}(\\boldsymbol{x})\\). Then in the higher-dimensional space, the data points become \\(\\{\\boldsymbol{\\phi}(\\boldsymbol{x}_1), \\boldsymbol{\\phi}(\\boldsymbol{x}_2), \\dots, \\boldsymbol{\\phi}(\\boldsymbol{x}_N)\\}\\). We will then build a data matrix each row of which represents a data points, and then build a covariance matrix \\(C\\) (assuming the features have a mean of \\(0\\) for now):\n\\[\\begin{equation*}\nC = \\frac{1}{N} \\sum_{i=1}^N \\boldsymbol{\\phi}(\\boldsymbol{x}_i) \\boldsymbol{\\phi}(\\boldsymbol{x}_i)^T\n\\end{equation*}\\]\nBased on the covariance matrix, we find its eigendecomposition:\n\\[\\begin{equation*}\nC \\boldsymbol{v}_i = \\lambda_i\\boldsymbol{v}_i\n\\end{equation*}\\]\nIf we directly work in the \\(D\\)-dimensional feature space, this will be computationally intractable. We will find a way to use the kernel, which computes the inner product in the higher-dimensional space by working in the original space. Note that the previous equation can be rewritten as\n\\[\\begin{equation*}\n\\frac{1}{N} \\sum_{i=1}^N \\boldsymbol{\\phi}(\\boldsymbol{x}_i) \\left(\\boldsymbol{\\phi}(\\boldsymbol{x}_i)^T \\boldsymbol{v}_i\\right) =  \\lambda_i\\boldsymbol{v}_i\n\\end{equation*}\\] by a substitution of \\(C\\). if \\(\\lambda_i&gt;0\\) for all \\(i\\), then we can write the eigenvectors as a linear combination of the features:\n\\[\\begin{equation*}\n\\boldsymbol{v}_i = \\sum_{n=1}^N a_{in} \\boldsymbol{\\phi}(\\boldsymbol{x}_n)\n\\end{equation*}\\]\nPlugging the linear combination back into the eigendecomposition equation above, and after some manipulation (see Exercise 8-2), we have\n\\[\\begin{equation*}\n\\frac{1}{N} \\sum_{n=1}^N k(\\boldsymbol{x}_l, \\boldsymbol{x}_n) \\sum_{m=1}^N a_{im} k(\\boldsymbol{x}_n, \\boldsymbol{x}_m) = \\lambda_i \\sum_{n=1}^N a_{in} k(\\boldsymbol{x}_l, \\boldsymbol{x}_n)\n\\end{equation*}\\]\nwhere \\(k\\) is the kernel function. In matrix notation, this can be written as (see Exercise 8-3)\n\\[\\begin{equation*}\nK^2\\boldsymbol{a}_i = \\lambda_i N K \\boldsymbol{a}_i\n\\end{equation*}\\] where \\(\\boldsymbol{a}_i = (a_{i1}, a_{i2}, \\dots, a_{iN})\\) and \\(K\\) is the kernel matrix. The solutions of which can be obtained by solving\n\\[\\begin{equation*}\nK\\boldsymbol{a}_i = \\lambda_i N \\boldsymbol{a}_i\n\\end{equation*}\\]\n(The solutions are not exactly the same, but they are the same corresponding to non-zero eigenvalues, which is enough). Here, we have a different normalization condition for \\(\\boldsymbol{a}_i\\) (not \\(\\boldsymbol{a}_i^T\\boldsymbol{a}_i=1\\)). Note that\n\\[\\begin{equation*}\n\\lambda_i N \\boldsymbol{a}_i^T\\boldsymbol{a}_i = \\boldsymbol{a}_i^T \\lambda_i N \\boldsymbol{a}_i = \\boldsymbol{a}_i^TK\\boldsymbol{a}_i = \\sum_{n=1}^N\\sum_{m=1}^N a_{in}a_{im} \\boldsymbol{\\phi}(\\boldsymbol{x}_n)^T\\boldsymbol{\\phi}(\\boldsymbol{x}_m) = \\boldsymbol{v}_i^T\\boldsymbol{v}_i = 1\n\\end{equation*}\\] Hence we have the normalization condition for \\(\\boldsymbol{a}_i\\):\n\\[\\begin{equation*}\n\\boldsymbol{a}_i^T\\boldsymbol{a}_i = \\frac{1}{\\lambda_i N}\n\\end{equation*}\\]\nWe can then project the original data \\(\\boldsymbol{x}_i\\) onto the principal components in the higher-dimensional feature space to obtain the reduced-dimensional representation by:\n\\[\\begin{equation*}\ny_i = \\boldsymbol{\\phi}(\\boldsymbol{x})^T\\boldsymbol{v}_i = \\sum_{j=1}^Na_{ij} \\boldsymbol{\\phi}(\\boldsymbol{x})^T\\boldsymbol{\\phi}(\\boldsymbol{x}_j) = \\sum_{j=1}^N a_{ij}k(\\boldsymbol{x}, \\boldsymbol{x}_j) \\quad 1\\le i\\le N\n\\end{equation*}\\]\nIn the above discussion, we assumed the projected features have zero means, which is not the case in general. To take this into account, let \\(\\bar{\\boldsymbol{\\phi}}(\\boldsymbol{x}_i)\\) be the mean-removed feature, which is obtained by\n\\[\\begin{equation*}\n\\boldsymbol{\\phi}(\\boldsymbol{x}_i) = \\boldsymbol{\\phi}(\\boldsymbol{x}_i) - \\frac{1}{N}\\sum_{j=1}^N\\boldsymbol{\\phi}(\\boldsymbol{x}_j)\n\\end{equation*}\\]\nThen the entries of the kernel matrix \\(\\bar{K}\\) is calculated by\n\\[\\begin{equation*}\n\\bar{K}_{ij} = \\bar{\\boldsymbol{\\phi}}(\\boldsymbol{x}_i)^T\\bar{\\boldsymbol{\\phi}}(\\boldsymbol{x}_j)\n\\end{equation*}\\]\nAfter some algebra, it can shown that (see Exercise 8-4)\n\\[\\begin{equation*}\n\\bar{K} = K -\\boldsymbol{1}_N K - K \\boldsymbol{1}_N + \\boldsymbol{1}_N K \\boldsymbol{1}_N\n\\end{equation*}\\] where \\(\\boldsymbol{1}_N\\) is a \\(N\\times N\\) matrix whose entries are all \\(1/N\\)\nExercise 8-2\nDerive the equation\n\\[\\begin{equation*}\n\\frac{1}{N} \\sum_{n=1}^N k(\\boldsymbol{x}_l, \\boldsymbol{x}_n) \\sum_{m=1}^N a_{im} k(\\boldsymbol{x}_n, \\boldsymbol{x}_m) = \\lambda_i \\sum_{n=1}^N a_{in} k(\\boldsymbol{x}_l, \\boldsymbol{x}_n)\n\\end{equation*}\\]\nExercise 8-3\nShow the covariance matrix in the higher dimension can be written as\n\\[\\begin{equation*}\nC^2\\boldsymbol{a}_i = \\lambda_i N C \\boldsymbol{a}_i\n\\end{equation*}\\]\nExample 8-5\nPerform KPCA on the concentric circles data.\n\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.datasets import make_circles\n\n# Generate non-linear data (e.g., concentric circles)\nX, y = make_circles(n_samples=400, factor=0.3, noise=0.05, random_state=32)\n\n# Plot the circle\nplt.scatter(X[y == 0, 0], X[y == 0, 1], color='red', label='Class 1')\nplt.scatter(X[y == 1, 0], X[y == 1, 1], color='blue', label='Class 2')\nplt.title('Original Circle data')\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n# Perform Kernel PCA\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=15)\nX_kpca = kpca.fit_transform(X)\n\n# Plot the results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_kpca[y == 0, 0], X_kpca[y == 0, 1], color='red', label='Class 1')\nplt.scatter(X_kpca[y == 1, 0], X_kpca[y == 1, 1], color='blue', label='Class 2')\nplt.title('Kernel PCA with RBF Kernel')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nIf we perform PCA on the concentric circle data, what will happen?\n\n# Perform PCA with two components\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Plot the results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_pca[y == 0, 0], X_pca[y == 0, 1], color='red', label='Class 1')\nplt.scatter(X_pca[y == 1, 0], X_pca[y == 1, 1], color='blue', label='Class 2')\nplt.title('PCA ')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nThe shape of the new coordinates are very similar to the original ones.\nExample 8-6\nPerform KPCA for the iris dataset to reduce the dimensions of the feature space to \\(2\\).\n\nfrom sklearn.preprocessing import StandardScaler\n\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\ntarget_names = iris.target_names\n\n# Standardize the data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Apply Kernel PCA with RBF kernel\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)\nX_kpca = kpca.fit_transform(X_scaled)\n\n# Plot the results of Kernel PCA\nplt.figure(figsize=(8, 6))\nscatter = plt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=y)\n# Create a custom legend\nhandles, labels = scatter.legend_elements(prop=\"colors\")\nlegend_labels = ['Setosa', 'Versicolor', 'Virginica']\nplt.legend(handles, legend_labels, title=\"Classes\");\nplt.title('Kernel PCA on Iris Dataset (RBF Kernel)')\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.show()\n\n\n\n\n\n\n\n\nExample 8-7\nApply and compare PCA and KPCA applied to the Swiss Roll Dataset.\n\nfrom sklearn.datasets import make_swiss_roll\n\n# Generate the Swiss roll dataset\nX, color = make_swiss_roll(n_samples=1000, noise=0.2, random_state=24)\n\n# Apply standard PCA\npca = PCA(n_components=2)\nX_pca = pca.fit_transform(X)\n\n# Apply Kernel PCA with RBF kernel\nkpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.02)\nX_kpca = kpca.fit_transform(X)\n\n# Plot the original Swiss roll in 3D\nfig = plt.figure(figsize=(12, 4))\nax = fig.add_subplot(131, projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color)\nax.set_title(\"Original Swiss Roll\")\n\n# Plot the results of standard PCA\nax = fig.add_subplot(132)\nplt.scatter(X_pca[:, 0], X_pca[:, 1], c=color)\nplt.title(\"PCA\")\n\n# Plot the results of Kernel PCA\nax = fig.add_subplot(133)\nplt.scatter(X_kpca[:, 0], X_kpca[:, 1], c=color)\nplt.title(\"Kernel PCA (RBF Kernel)\")\n\nplt.show()\n\n\n\n\n\n\n\n\nThe 2D projection from standard PCA flattens the Swiss roll, but it does not effectively capture the underlying spiral structure. Points that were far apart on the roll may end up close together in the 2D projection. On the other hand, Kernel PCA successfully unrolls the Swiss roll, revealing a structure that better preserves the original relationships between points. The colors (representing positions along the roll) form a smooth gradient, indicating that the non-linear structure has been effectively captured.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Dimension Reduction</span>"
    ]
  },
  {
    "objectID": "Chapter10_NeuralNetworks.html",
    "href": "Chapter10_NeuralNetworks.html",
    "title": "5  Neural Networks",
    "section": "",
    "text": "5.1 The Perceptron\nA perceptron has only an input layer and an output layer. As the figure shown below, we consider a perceptron with only one neuron in the output layer. The output neuron takes the weighted sum of the inputs (\\(w_1x_1+\\cdots+w_mx_m+b\\)), and applies an activation function \\(\\Gamma\\), usually the Heaviside step function, to the weighted sum to generate an output \\(y\\):\n\\[\\begin{equation*}\ny = \\Gamma(w_1x_1+\\cdots+w_mx_m+b)\n\\end{equation*}\\]\nwhere\n\\[\\begin{equation*}\n\\Gamma(x) =\n\\begin{cases}\n1 & \\text{if } x\\ge 1 \\\\\n0 & \\text{if } x &lt; 1\n\\end{cases}\n\\end{equation*}\\]\nIf the activation function \\(\\Gamma\\) is taken to be the identity (linear activation) function \\(\\Gamma(x)=x\\), then perceptron is reduced to a linear model. Perceptron is mainly used for binary classification. Training a perceptron model involves finding the optimal weights and biases. The choice of the activation function also matters. The We now use an example to illustrate how to implement perceptron in Python.\nExample 10-1\nUse perceptron for a random 2-class classification problem where the data are generated by sklearn.datasets.make_classification.\nimport numpy as np\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_classification\n\n# Generate a synthetic dataset\nX, y = make_classification(n_samples=200, n_features=2, n_informative=2, n_redundant=0, \n                           n_clusters_per_class=1, n_classes=2, random_state=50)\n\n# Initialize and train the perceptron model\nper_clf = Perceptron(max_iter=1000, tol=1e-3, random_state=50)\nper_clf.fit(X, y)\n\n# Make predictions\ny_pred = per_clf.predict(X)\n# Create a mesh grid for plotting the decision boundary\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.linspace(x_min, x_max, 200),\n                     np.linspace(y_min, y_max, 200))\n\n# Predict over the mesh grid\nZ = per_clf.predict(np.c_[xx.ravel(), yy.ravel()])\nZ = Z.reshape(xx.shape)\n\n# Plot the decision boundary\nplt.contourf(xx, yy, Z)\n\n# Plot the original data points\nplt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')\n\n# Add labels and title\nplt.xlabel('$x_1$')\nplt.ylabel('$x_2$')\nplt.title('Perceptron Decision Boundary');",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  },
  {
    "objectID": "Chapter10_NeuralNetworks.html#multilayer-perceptron-mlp",
    "href": "Chapter10_NeuralNetworks.html#multilayer-perceptron-mlp",
    "title": "5  Neural Networks",
    "section": "5.2 Multilayer Perceptron (MLP)",
    "text": "5.2 Multilayer Perceptron (MLP)\nIf we add one or more hidden layers to the perceptron structure, we obtain a multilayer perceptron (see the first figure above). With a more complex structure, MLP is capable of solving more difficult problems for both classification and regression. When the number of hidden layers is large, the model is called a deep neural network (DNN). Using DNNs to perform machine learning tasks is called deep learning (although there is no agreement on how many hidden layers is deep). The input layer takes the input features, each node of each hidden layer takes the weighted sum of the nodes from the previous layer and applies an activation function, and the output layer produces the output. There can be a single node in the output layer if the problem is a regression of binary classification. There can be more nodes for multi-class classification problems. For a multi-class classification problem, the activation function for the output layer is usually the softa function for a \\(K\\)-class classification problem is: \\[\n\\begin{aligned}\n\\Gamma(\\boldsymbol{x}) = \\Gamma(x_1, x_2, \\dots, x_K) = \\left(\\frac{e^{x_1}}{\\sum_{j=1}^K e^{x_j}}, \\frac{e^{x_2}}{\\sum_{j=1}^K e^{x_j}}, \\dots, \\frac{e^{x_K}}{\\sum_{j=1}^K e^{x_j}}\\right) \\in (0, 1)^K\n\\end{aligned}\n\\]\nThe training of an MLP involves finding the optimal weights and biases based on a selected loss function. The MSE function is natural for regression, and cross-entropy is appropriate for classification. To find the best hyperparameters, initial values (guesses) of the hyperparameters need to be first provided. Then the input layer receives the input features, and each node in the hidden layer closest to the input layer receives the weighted sum of the input nodes and applies the activation function. The subsequent hidden layers and the output layer follow the same process, and finally an output is produced corresponding to the initial hyperparameter values, and the loss function is evaluated at the output. This stage is called forward propagation. The second stage involves a process that reverses the previous one, called back propagation. Here, the gradient of the loss function with respect to the hyperparameters (weights and biases) are computed. With the gradient information, an efficient optimization algorithm such as stochastic gradient descent (SGD) can be used to search for the optimal hyperparameters. The core of the training of MLP is the back propagation. We use examples to illustrate how it works.\nExample 10-2\nIn the simple MLP for regression below, we have initialized all the weights and biases, and the input values for Nodes \\(I_1\\) and \\(I_2\\) are given. For both the hidden and output layers, suppose we use the sigmoid activation. Also assume the true target values are \\(0.01\\) and \\(0.99\\) for Nodes \\(O_1\\) and \\(O_2\\), respectively. Perform a forward pass and evaluate the value of the cost function.\n\nCalculate the weighted sum of the inputs \\(I_1\\) and \\(I_2\\) for \\(H_1\\), called the net input of \\(H_1\\): \\[\n\\begin{aligned}\n\\text{net}_{H_1} &= w_{1} \\cdot i_{1}+w_{2} \\cdot i_{2}+b_{1} \\\\\n\\text{net}_{H_1} &= 0.15 \\cdot 0.05+0.2 \\cdot 0.1+0.35=0.3775 \\\\\n\\end{aligned}\n\\] Use the sigmoid function to get the output of \\(H_1\\):\n\\[\\begin{equation*}\n\\text {out}_{H_1}=\\frac{1}{1+e^{-\\text{net}_{H_1}}}=\\frac{1}{1+e^{-0.3775}}=0.593269992\n\\end{equation*}\\]\nCarrying out the same process for \\(H_2\\):\n\\[\\begin{equation*}\n\\text{out}_{H_2}=0.596884378\n\\end{equation*}\\]\nRepeat this process for the nodes in the output layer, using the output from the hidden layer nodes as inputs.\nFor the output of \\(O_1\\), we have: \\[\n\\begin{aligned}\n\\text{net}_{O_1}&=w_{5} \\cdot \\text{out}_{H_1}+w_{6} \\cdot \\text {out}_{H_2}+b_{3} \\\\\n\\text{net}_{O_1}&=0.4 \\cdot 0.593269992+0.45 \\cdot 0.596884378+0.6=1.105905967 \\\\\n\\text{out}_{O_1}&=\\frac{1}{1+e^{-\\text{net}_{O_1}}}=\\frac{1}{1+e^{-1.105905967}}=0.75136507\n\\end{aligned}\n\\]\nSimilarly, for \\(O_2\\) we get:\n\\[\\begin{equation*}\n\\text{out}_{O_2}=0.772928465\n\\end{equation*}\\]\nWe can now calculate the error for each output node using the MSE function and sum them to get the total error:\n\\[\\begin{equation*}\nE_{\\text{total}}=\\sum \\frac{1}{2}(\\text{target}-\\text{output})^{2} = \\frac{1}{2}(0.01-0.75136507)^{2} + \\frac{1}{2}(0.99-0.772928465)^{2} = 0.298371109\n\\end{equation*}\\]\nExample 10-3\nNow we perform the back propagation process.\nOutput layer: Consider \\(w_5\\). We want to know how much a change in \\(w_5\\) affects the total error, i.e., \\(\\frac{\\partial \\text{E}_{\\text{total}} }{\\partial w_5}\\). Applying the chain rule, we know that\n\\[\\begin{equation*}\n\\frac{\\partial E_{\\text{total}}}{\\partial w_{5}}=\\frac{\\partial E_{\\text{total}}}{\\partial \\text{out}_{O_1}} \\cdot \\frac{\\partial \\text{out}_{O_1}}{\\partial \\text{net}_{O_1}} \\cdot \\frac{\\partial \\text{out}_{O_1}}{\\partial w_{5}}\n\\end{equation*}\\]\nWe now find each piece in this equation: \\[\n\\begin{aligned}\nE_{\\text{total}}&=\\frac{1}{2}\\left(\\text{target}_{o 1}-\\text { out }_{o 1}\\right)^{2}+\\frac{1}{2}\\left(\\operatorname{target}_{o 2}-\\text { out }_{o 2}\\right)^{2} \\\\\n\\frac{\\partial E_{\\text{total}}}{\\partial \\text{out}_{O_1}}&=-\\left(\\text{target}_{O_1}-\\text{out}_{O_1}\\right)=-(0.01-0.75136507)=0.74136507\n\\end{aligned}\n\\]\n\\[\\begin{equation*}\n\\begin{array}{l}\n\\text { out }_{o 1}=\\frac{1}{1+e^{-n e t_{o 1}}} \\\\\n\\frac{\\partial \\text { out }_{o 1}}{\\partial n e t_{o 1}}=\\operatorname{out}_{o 1}\\left(1-\\text { out }_{o 1}\\right)=0.75136507(1-0.75136507)=0.186815602\n\\end{array}\n\\end{equation*}\\]\n\\[\\begin{equation*}\n\\begin{array}{l}\n\\text { net }_{o 1}=w_{5} \\cdot \\text { out }_{h 1}+w_{6} \\cdot \\text { out }_{h 2}+b_{3} \\\\\n\\frac{\\partial\\text {net}_{O_1} }{\\partial w_{5}}=\\text { out }_{h 1}=0.593269992\n\\end{array}\n\\end{equation*}\\]\nPutting it all together: \\[\n\\begin{aligned}\n\\frac{\\partial E_{\\text{total}}}{\\partial w_{5}}&=\\frac{\\partial E_{\\text{total}}}{\\partial \\text{out}_{O_1}} \\cdot \\frac{\\partial \\text{out}_{O_1}}{\\partial \\text{net}_{O_1}} \\cdot \\frac{\\partial\\text{net}_{O_1}}{\\partial w_{5}} \\\\\n\\frac{\\partial E_{\\text{total}}}{\\partial w_{5}}&=0.74136507 \\cdot 0.186815602 \\cdot 0.593269992=0.082167041\n\\end{aligned}\n\\]\nSimilarly, we can obtain \\(\\frac{\\partial \\text{E}_{\\text{total}} }{\\partial w_6}\\), \\(\\frac{\\partial \\text{E}_{\\text{total}} }{\\partial w_7}\\), \\(\\frac{\\partial \\text{E}_{\\text{total}} }{\\partial w_8}\\).\nNow we deal with the hidden layer. First we find \\(\\frac{\\partial \\text{E}_{\\text{total}} }{\\partial w_1}\\) by\n\\[\\begin{equation*}\n\\frac{\\partial E_{\\text{total}}}{\\partial w_{1}}=\\frac{\\partial E_{\\text{total}}}{\\partial \\text{out}_{H_1}} \\cdot \\frac{\\partial \\text{out}_{H_1}}{\\partial \\text{net}_{H_1}} \\cdot \\frac{\\partial \\text{net}_{H_1}}{\\partial w_{1}}\n\\end{equation*}\\]\nNote \\(\\text{out}_{H_1}\\) affects both \\(\\text{out}_{O_1}\\) and \\(\\text{out}_{O_2}\\). So\n\\[\\begin{equation*}\n\\frac{\\partial E_{\\text{total}}}{\\partial \\text{out}_{H_1}}=\\frac{\\partial E_{O_1}}{\\partial \\text{out}_{H_1}}+\\frac{\\partial E_{O_2}}{\\partial \\text{out}_{H_1}}\n\\end{equation*}\\] and\n\\[\\begin{equation*}\n\\frac{\\partial E_{O_1}}{\\partial \\text {out}_{H_1}}=\\frac{\\partial E_{O_1}}{\\partial \\text{net}_{O_1}} \\cdot \\frac{\\partial \\text{net}_{O_1}}{\\partial \\text{out}_{H_1}} = \\frac{\\partial E_{O_1}}{\\partial \\text{out}_{O_1}} \\cdot \\frac{\\partial \\text {out}_{O_1}}{\\partial \\text{net}_{O_1}}\\cdot \\frac{\\partial \\text{net}_{O_1}}{\\partial \\text{out}_{H_1}} = 0.74136507\\cdot 0.186815602 \\cdot w_5 = 0.055399425\n\\end{equation*}\\]\nSimilarly, all the other quantities can be obtained. Once all the partial derivatives are obtained, SGD or other derivative-based optimization algorithm can be applied.\nExample 10-4\nWe perform MLP for the iris data set. Here we use two hidden layers, each with \\(10\\) neurons.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import confusion_matrix\n\n# Load the Iris dataset\niris = load_iris()\nX = iris.data\ny = iris.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=96)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and train the MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(12, 6), max_iter=5000, random_state=86)\nmlp.fit(X_train, y_train)\n\n# Make predictions\ny_pred = mlp.predict(X_test)\n\n# Evaluate the model\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n\n# Plotting the loss curve. The loss function continues to drop as the model is being trained\nplt.plot(mlp.loss_curve_)\nplt.title('MLP Training Loss Curve')\nplt.xlabel('Iterations')\nplt.ylabel('Loss');\n\nConfusion Matrix:\n [[20  0  0]\n [ 0 14  0]\n [ 0  0 11]]\n\n\n\n\n\n\n\n\n\nWe can check the optimal weights and biases of the model by:\n\n# See the weight matrix\nmlp.coefs_\n\n[array([[-0.38639999, -0.47921984, -0.85369411, -0.09692643, -0.10745231,\n          0.6204933 , -0.21007314, -0.42818818,  0.96059093, -0.33130668,\n         -0.53353584,  0.34266489],\n        [ 0.46759207,  0.43518618,  0.67255324, -0.64774433, -0.4716051 ,\n         -0.47048044, -0.29232437,  0.65049607, -0.56107454,  0.57628054,\n          0.43831958,  0.02490162],\n        [-0.28121689, -0.19595681, -0.89872946,  1.24731719,  0.53192312,\n         -1.76467841,  0.77030492, -0.27495727, -0.99168277, -0.79445805,\n         -0.38833726, -0.01354722],\n        [ 0.33654925,  0.20413659, -0.0604337 ,  0.53313068,  0.5098916 ,\n         -0.95091831,  1.26242908, -0.5976721 ,  0.17004848, -0.59130148,\n         -0.80525132,  0.21632437]]),\n array([[-5.22115239e-01, -1.01460772e-05,  6.63854561e-01,\n         -5.58638660e-01, -1.17290631e-01, -2.67314255e-01],\n        [-1.70766217e-01,  3.03615582e-06,  5.79073512e-01,\n         -4.28465918e-02,  4.48677979e-01, -3.98988864e-01],\n        [ 3.98023541e-01, -9.84730334e-07, -2.61874469e-01,\n         -2.00233714e-10,  8.13262572e-01, -2.68644639e-01],\n        [-3.08437189e-01, -3.39837556e-33, -1.15359523e+00,\n         -1.46528864e-01, -7.39587434e-01, -2.04793275e-04],\n        [-3.20656875e-02,  1.11437745e-20,  2.43235781e-01,\n         -1.88461832e-01, -1.36292845e+00, -1.62083433e-01],\n        [-6.06505850e-02, -5.57667625e-43,  1.92121725e+00,\n         -2.66795860e-01,  1.63185967e+00, -1.06931924e-02],\n        [ 4.66871444e-01, -2.90609369e-05, -1.57047082e+00,\n         -4.92826853e-01,  2.43951395e-01, -2.49797927e-04],\n        [-5.74238448e-05, -9.99580221e-05,  6.19060496e-01,\n          1.63528938e-01,  1.15051811e+00,  1.15172677e-01],\n        [-4.01003452e-01,  2.67118236e-13,  1.79810058e+00,\n          9.06125332e-02, -3.17692857e-01,  1.49533069e-06],\n        [-1.90776811e-16,  2.82001457e-16,  6.60208208e-01,\n         -1.05162652e-10,  1.11280488e+00,  7.64862740e-02],\n        [-3.90733121e-01, -4.16138798e-04,  6.93691343e-01,\n          1.39816374e-39,  5.97359729e-01,  3.77181784e-01],\n        [-5.27576431e-01, -4.57465937e-26, -1.34915774e-01,\n          4.11310602e-01, -6.20370127e-02,  1.09531065e-01]]),\n array([[ 6.57551335e-01,  6.76243565e-02, -5.44790501e-01],\n        [ 3.26215186e-07, -4.76841489e-04,  3.17413031e-05],\n        [ 1.74927927e-01,  1.01649310e+00, -1.90269507e+00],\n        [ 4.04607609e-01, -3.64797025e-01, -4.56734652e-01],\n        [ 7.86811483e-01, -4.27109606e-01,  1.51893780e-02],\n        [ 6.52852896e-02,  6.91145457e-01,  6.19429767e-01]])]\n\n\nThe first weight matrix is \\(4\\times 12\\), since we have four input neurons and \\(12\\) neurons in the first hidden layer. For example, the first row represents the weight from the first neuron of the input layer to all \\(12\\) neurons in the first hidden layer. For the biases, simply call:\n\n# Bias:\nmlp.intercepts_\n\n[array([ 1.064518  ,  1.00025039, -0.1233446 , -0.28436637,  1.38207967,\n         0.95564239, -0.32248468,  0.68590364,  0.24189182,  0.01015541,\n         0.90629805,  0.16900112]),\n array([ 0.30398812, -0.26807586,  0.58656479, -0.24204741, -0.25822549,\n        -0.56513263]),\n array([-1.71106822, -0.53736298,  1.27126756])]\n\n\nFor example, the first array of \\(12\\) elements represents the bias for the \\(12\\) neurons in the first hidden layer.\nExample 10-5\nApply MLP to the california housing dataset. Use two hidden layers each with \\(50\\) neurons.\n\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Load the Boston Housing dataset\ndata = fetch_california_housing()\nX = data.data\ny = data.target\n\n# Split the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=22)\n\n# Standardize the features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Initialize and train the MLPRegressor\nmlp = MLPRegressor(hidden_layer_sizes=(50, 50), max_iter=1000, random_state=77)\nmlp.fit(X_train, y_train)\n\n# Make predictions\ny_pred = mlp.predict(X_test)\n\n# Evaluate the model\nmse = mean_squared_error(y_test, y_pred)\nr2 = r2_score(y_test, y_pred)\n\nprint(f\"Mean Squared Error: {mse:.2f}\")\nprint(f\"R^2 Score: {r2:.2f}\")\n\n# Plot the predictions vs actual values\nplt.scatter(y_test, y_pred)\nplt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], 'r--')\nplt.xlabel('Actual Prices')\nplt.ylabel('Predicted Prices')\nplt.title('MLP Regression: Actual vs Predicted Prices')\nplt.show()\n\n# Plot the loss curve\nplt.plot(mlp.loss_curve_)\nplt.title('MLP Training Loss Curve')\nplt.xlabel('Iterations')\nplt.ylabel('Loss')\nplt.show()\n\nMean Squared Error: 0.26\nR^2 Score: 0.80",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Neural Networks</span>"
    ]
  }
]